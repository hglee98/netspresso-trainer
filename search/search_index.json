{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NetsPresso Trainer is a PyTorch training repository specialized for model training with torch.fx graphmodule conversion and automatic compression.  </p> <p>By using NetsPresso Trainer and NetsPresso together, you can take advantage of features such as model compression without significant performance drop, ONNX export, and NVIDIA TensorRT export.</p> <p>Enjoy training and deploying your own edge AI models with NetsPresso Trainer!</p>"},{"location":"benchmarks/benchmarks/","title":"Benchmarks","text":"<p>We are working on creating pretrained weights with NetsPresso Trainer and our own resources. We base training recipes on the official repositories or original papers to replicate the performance of models.</p> <p>For models that we have not yet trained with NetsPresso Trainer, we provide their pretrained weights from other awesome repositories. We have converted several models' weights into our own model architectures. We appreciate all the original authors and we also do our best to make other values.</p> <p>Therefore, in the benchmark performance table of this section, a Reproduced status of True indicates performance obtained from our own training resources. In contrast, a False status means that the data is from original papers or repositories.</p>"},{"location":"benchmarks/benchmarks/#classification","title":"Classification","text":"Dataset Model Weights Resolution Acc@1 Acc@5 Params MACs torch.fx NetsPresso Reproduced Remarks ImageNet1K EfficientFormer-l1 download 224x224 80.20 - 12.30M 1.30G Supported Supported False - ImageNet1K MixNet-s download 224x224 75.13 - - - Supported Supported False - ImageNet1K MixNet-m download 224x224 76.49 - - - Supported Supported False - ImageNet1K MixNet-l download 224x224 78.67 - - - Supported Supported False - ImageNet1K MobileNetV3-small download 224x224 67.67 87.40 2.50M 0.03G Supported Supported False - ImageNet1K MobileViT download 224x224 78.40 - 5.60M - Supported Supported False - ImageNet1K ResNet18 download 224x224 68.47 88.20 11.69M 1.82G Supported Supported True - ImageNet1K ResNet34 download 224x224 72.26 90.63 21.80M 3.67G Supported Supported True - ImageNet1K ResNet50 download 224x224 79.61 94.67 25.56M 2.62G Supported Supported True - ImageNet1K ViT-tiny download 224x224 72.91 - 5.70M - Supported Supported False -"},{"location":"benchmarks/benchmarks/#semantic-segmentation","title":"Semantic segmentation","text":"Dataset Model Weights Resolution mIoU Pixel acc Params MACs torch.fx NetsPresso Reproduced Remarks - SegFormer-b0 download - - - - - Supported Supported False - Cityscapes PIDNet-s download 2048x1024 78.8 - - - Supported Supported False -"},{"location":"benchmarks/benchmarks/#object-detection","title":"Object detection","text":"Dataset Model Weights Resolution mAP50 mAP75 mAP50:95 Params MACs torch.fx NetsPresso Reproduced Remarks COCO YOLOX-s download 640x640 58.56 44.10 40.63 8.97M 13.40G Supported Supported True conf_thresh=0.01, nms_thresh=0.65"},{"location":"benchmarks/benchmarks/#acknowledgment","title":"Acknowledgment","text":"<p>The original weight files which are not yet trained with NetsPresso Trainer are as follows.</p> <ul> <li>EfficientFormer: apple/ml-cvnets</li> <li>MobileViT: apple/ml-cvnets</li> <li>ViT-tiny: apple/ml-cvnets</li> <li>SegFormer: (Hugging Face) nvidia </li> <li>PIDNet: XuJiacong/PIDNet</li> </ul>"},{"location":"components/data/","title":"Data","text":"<p>NetsPresso Trainer supports learning functions for various vision tasks with your custom data.  In addition to data stored in a local repository, it also supports learning with data accessible through APIs such as Hugging Face datasets.  Currently, the dataset formats supported by NetsPresso Trainer are fixed in a specific form, but we plan to expand to more dataset formats such as COCO format in the future.  </p> <p>On this page, we will guide you on the data format you need to learn with your custom data and how to learn using Hugging Face datasets. </p>"},{"location":"components/data/#local-custom-datasets","title":"Local custom datasets","text":""},{"location":"components/data/#supporting-image-formats","title":"Supporting image formats","text":"<p>For image data, various extension images are supported, but we recommend one of <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code>, and <code>.bmp</code>. In this case, label data used in semantic segmentation must be saved as <code>.png</code> to prevent data loss and utilize image header information.  The following sections introduce how to organize data for each task. </p>"},{"location":"components/data/#common-configuration","title":"Common configuration","text":"<p>Regardless of the task, dataset directory should be organized as follows:</p> <ul> <li>Train: This directory should contain all the training images and corresponding label files.</li> <li>Validation: This directory should house validation images and their corresponding labels, used to tune the hyperparameters.</li> <li>Test: This directory should include test images and labels for final model evaluation.</li> </ul> <p>This structure should be reflected in your configuration file under the respective paths.</p> Field  Description <code>data.name</code> (str) The name of dataset. <code>data.task</code> (str) <code>classification</code> for image classification, <code>segmentation</code> for semantic segmentation, and <code>detection</code> for object detection. <code>data.format</code> <code>local</code> as an identifier of dataset format. <code>data.path.root</code> (str) Root directory of dataset. <code>data.path.train.image</code> (str) The directory for training images. Should be relative path to root directory. <code>data.path.valid.image</code> (str) The directory for validation images. Should be relative path to root directory. <code>data.path.test.image</code> (str) The directory for test images. Should be relative path to root directory."},{"location":"components/data/#image-classification","title":"Image classification","text":"<p>To train an image classification model using NetsPresso Trainer, users must organize their data according to a specified format.</p> <ul> <li>train images must be in same directory.</li> <li>validation images must be in same directory.</li> <li>labels for images are given by csv file. The csv file contains image file name and correspoinding class label.</li> </ul> Field  Description <code>data.id_mapping</code> (list) Class list for each class index. <code>data.path.train.label</code> (str) For classificaiton, label field must be path of <code>.csv</code> file. This should be relative path to root directory. <code>data.path.valid.label</code> (str) For classificaiton, label field must be path of <code>.csv</code> file. This should be relative path to root directory. <code>data.path.test.label</code> (str) For classificaiton, label field must be path of <code>.csv</code> file. This should be relative path to root directory. Data hierarchy example - ImageNet1K <pre><code>data/imagenet1k\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10026.JPEG\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10027.JPEG\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10029.JPEG\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 valid\n\u2502       \u251c\u2500\u2500 ILSVRC2012_val_00000001.JPEG\n\u2502       \u251c\u2500\u2500 ILSVRC2012_val_00000002.JPEG\n\u2502       \u251c\u2500\u2500 ILSVRC2012_val_00000003.JPEG\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 imagenet_train.csv\n    \u2514\u2500\u2500 imagenet_valid.csv\n</code></pre> Label csv example - ImageNet1K <pre><code>| image_id             | class    |\n|----------------------|----------|\n| n03792972_3671.JPEG  | 728      |\n| n04357314_4256.JPEG  | 810      |\n| n02965783_127.JPEG   | 576      |\n| n04465501_16825.JPEG | 289      |\n| n09246464_5059.JPEG  | 359      |\n| ... | ... |\n</code></pre> Data configuration example - ImageNet1K <pre><code>data:\n  name: imagenet1k\n  task: classification\n  format: local # local, huggingface\n  path:\n    root: path_to/IMAGENET1K # dataset root\n    train:\n      image: images/train # directory for training images\n      label: labels/imagenet_train.csv  # label for training images\n    valid:\n      image: images/valid  # directory for valid images\n      label: labels/imagenet_valid.csv  # label for valid images\n    test:\n      image: ~  # directory for test images\n      label: ~  # label for test images\n  id_mapping: [\"kit fox\", \"English setter\", \"Siberian husky\", \"Australian terrier\", ...]\n</code></pre>"},{"location":"components/data/#semantic-segmentation","title":"Semantic segmentation","text":"<p>To train a semantic segmentation model using NetsPresso Trainer, the data must be in the following formats: </p> <ul> <li>For each training image, there must be a label file (image) indicating the original image and the class index of each pixel of the image.</li> <li>Users must create an image and label directory under the root directory and put the corresponding files in each directory.</li> <li>In this case, training data and validation data can be distinguished in different directories. For example, training data can be placed in train/image, train/label directories, and validation data can be placed in valid/image, valid/label directories.</li> <li>Users must know the class name corresponding to each pixel value (RGB or L (grayscale) format) in the label file.</li> </ul> Field  Description <code>data.label_image_mode</code> (str) Image mode to convert the label. Should be one of <code>RGB</code>, <code>L</code>, and <code>P</code>. This field is not case-sensitive. <code>data.path.train.label</code> (str) For segmentation, label field must be path of label directory. This should be relative path to root directory. <code>data.path.valid.label</code> (str) For segmentation, label field must be path of label directory. This should be relative path to root directory. <code>data.path.test.label</code> (str) For segmentation, label field must be path of label directory. This should be relative path to root directory. <code>data.id_mapping</code> (dict, list) Key-value pair between label value (<code>RGB</code>, <code>L</code>, or <code>P</code>) and class name. Should be a dict of {label_value: classname} or a list of class names whose indices are same with the label value (image_mode: <code>L</code> or <code>P</code>). <code>data.palette</code> (dict) Color mapping for visualization. If <code>none</code>, automatically select the color for each class. Data hierarchy example - PascalVOC 2012 <pre><code>data/voc2012_seg\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 2007_000032.jpg\n\u2502   \u2502   \u251c\u2500\u2500 2007_000039.jpg\n\u2502   \u2502   \u251c\u2500\u2500 2007_000063.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 valid\n\u2502       \u251c\u2500\u2500 2007_000033.jpg\n\u2502       \u251c\u2500\u2500 2007_000042.jpg\n\u2502       \u251c\u2500\u2500 2007_000061.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 2007_000032.png\n    \u2502   \u251c\u2500\u2500 2007_000039.png\n    \u2502   \u251c\u2500\u2500 2007_000063.png\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 valid\n        \u251c\u2500\u2500 2007_000033.png\n        \u251c\u2500\u2500 2007_000042.png\n        \u251c\u2500\u2500 2007_000061.png\n        \u2514\u2500\u2500 ...\n</code></pre> Data configuration example - PascalVOC 2012 <pre><code>data:\n  name: voc2012\n  task: segmentation\n  format: local\n  path:\n    root: path_to/VOC12Dataset\n    train:\n      image: image/train\n      label: mask/train\n    valid:\n      image: image/valid\n      label: mask/valid\n    test:\n      image: ~  # directory for test images\n      label: ~  # directory for test labels\n    pattern:\n      image: ~\n      label: ~\n  label_image_mode: RGB\n  id_mapping:\n    (0, 0, 0): background\n    (128, 0, 0): aeroplane\n    (0, 128, 0): bicycle\n    (128, 128, 0): bird\n    (0, 0, 128): boat\n    (128, 0, 128): bottle\n    (0, 128, 128): bus\n    (128, 128, 128): car\n    (64, 0, 0): cat\n    (192, 0, 0): chair\n    (64, 128, 0): cow\n    (192, 128, 0): diningtable\n    (64, 0, 128): dog\n    (192, 0, 128): horse\n    (64, 128, 128): motorbike\n    (192, 128, 128): person\n    (0, 64, 0): pottedplant\n    (128, 64, 0): sheep\n    (0, 192, 0): sofa\n    (128, 192, 0): train\n    (0, 64, 128): tvmonitor\n    (128, 64, 128): void\n  pallete: ~\n</code></pre>"},{"location":"components/data/#object-detection","title":"Object detection","text":"<p>To train an object detection model using NetsPresso Trainer, the data must be in the following formats: </p> <ul> <li>For object detection model training, each training image must have a corresponding <code>.txt</code> file indicating the original image and the bounding box and class index corresponding to each bounding box of the image.</li> <li>The format of the bounding box follows the YOLO dataset format <code>[x_center, y_center, width, height]</code> (normalized).</li> <li>Each <code>.txt</code> file must contain one line for each bounding box.</li> <li>In this case, training data and validation data can be distinguished in different directories. For example, training data can be placed in train/image, train/label directories, and validation data can be placed in valid/image, valid/label directories.</li> <li>Users must know the class name corresponding to each class index in the label file.</li> </ul> Field  Description <code>data.path.train.label</code> (str) For detection, label field must be path of label directory. This should be relative path to root directory. <code>data.path.valid.label</code> (str) For detection, label field must be path of label directory. This should be relative path to root directory. <code>data.path.test.label</code> (str) For detection, label field must be path of label directory. This should be relative path to root directory. <code>data.id_mapping</code> (list) Class list for each class index. <code>data.palette</code> (dict) Color mapping for visualization. If <code>none</code>, automatically select the color for each class. Data hierarchy example - COCO 2017 <pre><code>data/coco2017\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 000000000009.jpg\n\u2502   \u2502   \u251c\u2500\u2500 000000000025.jpg\n\u2502   \u2502   \u251c\u2500\u2500 000000000030.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 valid\n\u2502       \u251c\u2500\u2500 000000000139.jpg\n\u2502       \u251c\u2500\u2500 000000000285.jpg\n\u2502       \u251c\u2500\u2500 000000000632.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 000000000009.txt\n    \u2502   \u251c\u2500\u2500 000000000025.txt\n    \u2502   \u251c\u2500\u2500 000000000030.txt\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 valid\n        \u251c\u2500\u2500 000000000139.txt\n        \u251c\u2500\u2500 000000000285.txt\n        \u251c\u2500\u2500 000000000632.txt\n        \u2514\u2500\u2500 ...\n</code></pre> Label txt example - COCO 2017 <pre><code>58 0.389578125 0.4161032863849765 0.038593749999999996 0.16314553990610328\n62 0.127640625 0.5051525821596244 0.23331249999999998 0.22269953051643193\n62 0.9341953125 0.583462441314554 0.127109375 0.18481220657276995\n56 0.60465625 0.6325469483568076 0.0875 0.24138497652582158\n56 0.5025078125 0.6273239436619719 0.096609375 0.2311737089201878\n56 0.6691953125 0.6189906103286384 0.047140625000000005 0.19098591549295774\n56 0.512796875 0.5282511737089202 0.03371875 0.02720657276995305\n0 0.6864453125 0.5319600938967136 0.082890625 0.3239671361502347\n0 0.612484375 0.4461971830985916 0.023625 0.08389671361502347\n68 0.811859375 0.5017253521126761 0.02303125 0.037488262910798126\n72 0.7863203125 0.5363732394366197 0.031703125 0.2542488262910798\n73 0.9561562499999999 0.7717018779342724 0.02240625 0.10730046948356808\n73 0.96825 0.7780751173708921 0.020125 0.10901408450704225\n74 0.7105546875 0.31 0.021828125 0.05136150234741784\n75 0.8865624999999999 0.8316079812206573 0.0573125 0.2104929577464789\n75 0.5569453125 0.5167018779342724 0.017765625 0.05293427230046949\n56 0.6516640625 0.5288262910798122 0.015046875000000001 0.029389671361502348\n75 0.388046875 0.4784154929577465 0.022218750000000002 0.04138497652582159\n75 0.5338359375 0.48794600938967136 0.015203125000000001 0.039272300469483566\n60 0.599984375 0.6471478873239437 0.19618750000000001 0.20875586854460096\n</code></pre> Custom object detection dataset example - COCO 2017 <pre><code>data:\n  name: coco2017\n  task: detection\n  format: local # local, huggingface\n  path:\n    root: ./data/coco2017 # dataset root\n    train:\n      image: images/train # directory for training images\n      label: labels/train # directory for training labels\n    valid:\n      image: images/valid # directory for valid images\n      label: labels/valid # directory for valid labels\n    test:\n      image: ~\n      label: ~\n    pattern:\n      image: ~\n      label: ~\n  id_mapping: ['person', 'bicycle', 'car', ...]\n  pallete: ~\n</code></pre>"},{"location":"components/data/#hugging-face-datasets","title":"Hugging Face datasets","text":"<p>NetsPresso Trainer is striving to support various dataset hubs and platforms.  As part of that effort and first step, NetsPresso Trainer can be used with data in Hugging Face datasets. </p> Field  Description <code>data.name</code> (str) The name of dataset. <code>data.task</code> (str) <code>classification</code> for image classification, <code>segmentation</code> for semantic segmentation, and <code>detection</code> for object detection. <code>data.format</code> <code>huggingface</code> as an identifier of dataset format. <code>data.metadata.custom_cache_dir</code> (str) Cache directory to load and save dataset files from Hugging Face. <code>data.metadata.repo</code> (str) Repository name. (e.g. <code>competitions/aiornot</code> represents the dataset <code>huggingface.co/datasets/competitions/aiornot</code>.) <code>data.metadata.subset</code> (str, optional) Subset name if the dataset contains multiple versions. <code>data.metadata.features.image</code> (str) The key representing the image at the dataset header. <code>data.metadata.features.label</code> (str) The key representing the label at the dataset header. Huggingface dataset example - beans <pre><code>data:\n  name: beans\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: beans\n    subset: ~\n    features:\n      image: image\n      label: labels\n</code></pre>"},{"location":"components/environment/","title":"Environment","text":"<p>The environment configuration contains values that determine the training environment, such as the number of workers for multi-processing and the GPU ids to be used. The following yaml is the environment configuration example.</p> <pre><code>environment: \n  seed: 1\n  batch_size: 8\n  num_workers: 4 \n  gpus: 0, 1, 2, 3\n</code></pre>"},{"location":"components/environment/#field-list","title":"Field list","text":"Field  Description <code>environment.seed</code> (int) Random seed. <code>environment.batch_size</code> (int) The number of samples in single batch input. <code>environment.num_workers</code> (int) The number of multi-processing workers to be used by the data loader. <code>environment.gpus</code> (str) GPU ids to use, this should be separated by commas."},{"location":"components/logging/","title":"Logging","text":"<p>NetsPresso Trainer provides training results in a variety of multiple formats. As a following example, users can determine most of output formats through boolean flags, and can adjust the intervals of evaluations and checkpoint saves with a simple configuration.</p> <pre><code>logging:\n  project_id: ~\n  output_dir: ./outputs\n  tensorboard: true\n  csv: true\n  image: true\n  stdout: true\n  save_optimizer_state: true\n  validation_epoch: &amp;validation_epoch 5\n  save_checkpoint_epoch: *validation_epoch  # Multiplier of `validation_epoch`.\n</code></pre>"},{"location":"components/logging/#tensorboard","title":"Tensorboard","text":"<p>We provide basic tensorboard to track your training status. Run the tensorboard with the following command: </p> <pre><code>tensorboard --logdir ./outputs --port 50001 --bind_all\n</code></pre> <p>Note that the default directory of saving result will be <code>./outputs</code> directory. The port number <code>50001</code> is same with the port forwarded in example docker setup. You can change with any port number available in your environment.</p>"},{"location":"components/logging/#field-list","title":"Field list","text":"Field  Description <code>logging.project_id</code> (str) Project name to save the experiment. If <code>None</code>, it is set as <code>{task}_{model}</code> (e.g. <code>segmentation_segformer</code>). <code>logging.output_dir</code> (str) Root directory for saving the experiment. Default location is <code>./outputs</code>. <code>logging.tensorboard</code> (bool) Whether to use the tensorboard. <code>logging.csv</code> (bool) Whether to save the result in csv format. <code>logging.image</code> (bool) Whether to save the validation results. It is ignored if the task is <code>classification</code>. <code>logging.stdout</code> (bool) Whether to log the standard output. <code>logging.save_optimizer_state</code> (bool) Whether to save optimizer state with model checkpoint to resume training. <code>logging.validation_epoch</code> (int) Validation frequency in total training process. <code>logging.save_checkpoint_epoch</code> (int) Checkpoint saving frequency in total training process."},{"location":"components/overview/","title":"Overview","text":"<p>NetsPresso Trainer and NetsPresso service provide a convenient experience in training, compressing, retraining, and deploying models for user devices, seamlessly. In that process, NetsPresso Trainer manages both training and retraining phases, ensuring the models are fully compatible with NetsPresso.</p> <p>NetsPresso Trainer categorizes essential parameters for training into six configuration modules. Each module is responsible for the following aspects:</p> <ul> <li>Data: Defines the structure of the user-customized or Hugging Face datasets for interpretation by NetsPresso Trainer.</li> <li>Augmentation: Defines the data augmentation recipe.</li> <li>Model: Defines the model architecture, loss modules, and pretrained weights.</li> <li>Training: Defines necessary elements like optimizer, epochs, and batch size for training.</li> <li>Logging: Defines output formats of training results.</li> <li>Environment: Defines the training environment, including GPU usage and dataloader multi-processing.</li> </ul> <p>This component section describes in detail the six configuration modules which are necessary to use NetsPresso Trainer. You can see yaml configuration examples in our public repository.</p>"},{"location":"components/overview/#advantage-of-netspresso-trainer","title":"Advantage of NetsPresso Trainer","text":""},{"location":"components/overview/#use-sota-models-fully-compatible-with-netspresso","title":"Use SOTA models fully compatible with NetsPresso","text":"<p>NetsPresso Trainer provides reimplemented SOTA models that ensure compatibility with NetsPresso. This allows users to avoid expending resources on changing model formats for model compression and device deployment. Therefore, the users can easily utilize SOTA models to their applications.</p>"},{"location":"components/overview/#easily-trainable-with-yaml-configuration","title":"Easily trainable with yaml configuration","text":"<p>NetsPresso Trainer encapsulates all the necessary values within configurations for model training. This enables extensive optimization attempts with mere modifications of these configuration files. Also, this enhances usability by using same configuration format for retraining compressed models.</p>"},{"location":"components/augmentation/overview/","title":"Augmentation - Overview","text":"<p>NetsPresso Trainer provides data augmentation functions to improve model performance, allowing users to configure their own training recipes as desired.  Data augmentation in NetsPresso Trainer is based on torch and torchvision, and all augmentations are implemented based on <code>pillow</code> images.</p> <p>In NetsPresso Trainer, users can create their desired augmentation recipe by composing a configuration as below. We separately define sample transform procedures for training and inference. </p> <p>Functions specified in the <code>train</code> and <code>inference</code> fields are applied sequentially as listed. Note that after all image processing is completed, the final image size must match with the size specified in <code>augmentation.img_size</code>.</p> <pre><code>augmentation:\n  img_size: &amp;img_size 256\n  train:\n    - \n      name: randomresizedcrop\n      size: *img_size\n      scale: [0.08, 1.0]\n      ratio: [0.75, 1.33]\n      interpolation: bilinear\n    - \n      name: randomhorizontalflip\n      p: 0.5\n    -\n      name: mixing\n      mixup: [0.25, 1.0]\n      cutmix: ~\n      inplace: false\n  inference:\n    - \n      name: resize\n      size: [*img_size, *img_size]\n      interpolation: bilinear\n      max_size: ~\n</code></pre>"},{"location":"components/augmentation/overview/#gradio-demo-for-simulating-the-transform","title":"Gradio demo for simulating the transform","text":"<p>In many learning function repositories, it is recommended to read the code and documentation or actually run the training to check the logs to see how augmentations are performed.  NetsPresso Trainer supports augmentation simulation to help users easily understand the augmentation recipe they have configured.  By copying and pasting the augmentation configuration into the simulator, users can preview how a specific image will be augmented in advance. However, transforms (e.g. Normalize, ToTensor) used to convert the image array for learning purposes are excluded from the simulation visualization process.  In particular, since this simulator directly imports the augmentation modules used in NetsPresso Trainer, users can use the same functions as the augmentation functions used in actual training to verify the results.  </p> <p>Our team hopes that the learning process with NetsPresso Trainer will become a more enjoyable experience for all users. </p>"},{"location":"components/augmentation/overview/#running-on-your-environment","title":"Running on your environment","text":"<p>Please run the gradio demo with following command:</p> <pre><code>bash scripts/run_simulator_augmentation.sh\n</code></pre>"},{"location":"components/augmentation/overview/#field-list","title":"Field list","text":"Field  Description <code>augmentation.img_size</code> (int) The image size of model input after finishing the data augmentation <code>augmentation.train</code> list[dict] List of transform functions for training. Augmentation process is defined on list order. <code>augmentation.inference</code> (list[dict]) List of transform functions for inference. Augmentation process is defined on list order."},{"location":"components/augmentation/transforms/","title":"Transforms","text":"<p>Users can easily create their own augmentation recipe simply by listing their desired data transform modules. It's possible to adjust the intensity and frequency of each transform module, and the listed transform modules are applied in sequence to produce augmented data. In NetsPresso Trainer, a visualization tool is also provided through a Gradio demo, allowing users to see how their custom augmentation recipe produces the data for the model.</p>"},{"location":"components/augmentation/transforms/#supporting-transforms","title":"Supporting transforms","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/augmentation/transforms/#centercrop","title":"CenterCrop","text":"<p>This augmentation follows the CenterCrop in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"centercrop\" to use <code>CenterCrop</code> transform. <code>size</code> (int or list) Desired output size of the crop. If size is an int, a square crop (size, size) is made. If provided a list of length 1, it will be interpreted as (size[0], size[0]). If a list of length 2 is provided, a square crop (size[0], size[1]) is made. CenterCrop example <pre><code>augmentation:\n  train:\n    - \n      name: centercrop\n      size: 224\n</code></pre>"},{"location":"components/augmentation/transforms/#colorjitter","title":"ColorJitter","text":"<p>This augmentation follows the ColorJitter in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"colorjitter\" to use <code>ColorJitter</code> transform. <code>brightness</code> (float or list) The brightness scale value is randomly selected within [max(0, 1 - brightness), 1 + brightness] or given [min, max] range. <code>contrast</code> (float or list) The contrast scale value is randomly selected within [max(0, 1 - contrast), 1 + contrast] or given [min, max] range. <code>saturation</code> (float or list) The saturation scale value is randomly selected within [max(0, 1 - saturation), 1 + saturation] or given [min, max] range. <code>hue</code> (float or list) The hue scale value is randomly selected within [max(0, 1 - hue), 1 + hue] or given [min, max] range. <code>p</code> (float) The probability of applying the color jitter. If set to <code>1.0</code>, the color transform is always applied. ColorJitter example <pre><code>augmentation:\n  train:\n    - \n      name: colorjitter\n      brightness: 0.25\n      contrast: 0.25\n      saturation: 0.25\n      hue: 0.1\n      p: 1.0\n</code></pre>"},{"location":"components/augmentation/transforms/#hsvjitter","title":"HSVJitter","text":"<p>HSVJitter is based on <code>augment_hsv</code> function of YOLOX repository. This transform convert input image to HSV format, and randomly adjust according to magnitude configuration. Each channel can be randomly adjusted or remain unchanged for every transform step.</p> Field  Description <code>name</code> (str) Name must be \"hsvjitter\" to use <code>HSVJitter</code> transform. <code>h_mag</code> (int) Randomly adjust the H channel within the range of [-h_mag, h_mag]. <code>s_mag</code> (int) Randomly adjust the S channel within the range of [-s_mag, s_mag]. <code>v_mag</code> (int) Randomly adjust the V channel within the range of [-v_mag, v_mag]. HSVJitter example <pre><code>augmentation:\n  train:\n    -\n      name: hsvjitter\n      h_mag: 5\n      s_mag: 30\n      v_mag: 30\n</code></pre>"},{"location":"components/augmentation/transforms/#mixing","title":"Mixing","text":"<p>We defined Mixing transform as the combination of CutMix and MixUp augmentation. This shuffles samples within a batch instead of processing per image. Therefore, Mixing transform must be in the last function of augmentation racipe if user wants to use it. Also, Mixing not assumes a batch size 1. If both MixUp and CutMix are activated, only one of two is randomly selected and used per batch processing.</p> <p>Cutmix augmentation is based on CutMix: Regularization strategy to train strong classifiers with localizable features and MixUp augmentation is based on mixup: Beyond empirical risk minimization. These implementation follow the RandomCutmix and RandomMixup in the ml-cvnets library.</p> <p>Currently, NetsPresso Trainer does not support a Gradio demo visualization for Mixing. This feature is planned to be added soon.</p> Field  Description <code>name</code> (str) Name must be \"mixing\" to use <code>Mixing</code> transform. <code>mixup</code> (list[float], optional) List of length 2 which contains [mixup alpha, applying probability]. If None, mixup is not applied. <code>cutmix</code> (list[float], optional) List of length 2 which contains [cutmix alpha, applying probability]. If None, cutmix is not applied. <code>inplace</code> (bool) Whether to operate as inplace. Mixing example <pre><code>augmentation:\n  train:\n    -\n      name: mixing\n      mixup: [0.25, 1.0]\n      cutmix: ~\n      inplace: false\n</code></pre>"},{"location":"components/augmentation/transforms/#mosaicdetection","title":"MosaicDetection","text":"<p>This MosaicDetection augmentation is based on YOLOX repository. For each sample, the following steps are taken to create an augmented sample.</p> <ul> <li>Load three additional images.</li> <li>Resize the four images to fit the <code>size</code>.</li> <li>Merge the four images into one, with the merge center point randomly determined.</li> <li>Apply a random affine transformation to the merged image. And resize output image to fit the <code>size</code>.</li> <li>Finally, if <code>enable_mixup</code> is set to <code>True</code>, apply a mixup transformation with a fixed alpha of 0.5. The mixup image also is randomly loaded from dataset.</li> </ul> Field  Description <code>name</code> (str) Name must be \"mosaicdetection\" to use <code>MosaicDetection</code> transform. <code>size</code> (list) Desired output size of the <code>MosaicDetection</code>. <code>mosaic_prob</code> (float) The probability of applying the <code>MosaicDetection</code>. If set to 1.0, it is always applied. <code>affine_scale</code> (list) Generate affine matrix with a scale range of [affine_scale[0], affine_scale[1]]. <code>degrees</code> (float) Generate affine matrix with a rotation range of [-degrees, degrees]. <code>translate</code> (float) Generate affine matrix with a translate range of [-translate, translate]. <code>shear</code> (float) Generate affine matrix with a shear range of [-shear, shear]. Randomly generate for each x-axis and y-axis. <code>enable_mixup</code> (bool) Whether to apply mixup. <code>mixup_prob</code> (float) The probability of applying the mixup. If set to 1.0, it is always applied. <code>mixup_scale</code> (list) Resize scale range for mixup image. <code>fill</code> (int)  This is used to fill pixels with constant value. <code>mosaic_off_epoch</code> (int) Turn off the <code>MosaicDetection</code> at <code>mosaic_off_epoch</code> epoch. MosaicDetection example <pre><code>augmentation:\n  train:\n    -\n      name: mosaicdetection\n      size: [*img_size, *img_size]\n      mosaic_prob: 1.0\n      affine_scale: [0.5, 1.5]\n      degrees: 10.0\n      translate: 0.1\n      shear: 2.0\n      enable_mixup: True\n      mixup_prob: 1.0\n      mixup_scale: [0.5, 1.5]\n      fill: 114\n      mosaic_off_epoch: 10\n</code></pre>"},{"location":"components/augmentation/transforms/#pad","title":"Pad","text":"<p>Pad an image with constant. This augmentation is based on the Pad in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"pad\" to use <code>Pad</code> transform. <code>size</code> (int or list) Padding on each border. If a single int is provided, target size is (<code>size</code>, <code>size</code>). If a list is provided, it must be length 2, and will produce size of (<code>size[0]</code>, <code>size[1]</code>) padded image. If each edge of input image is greater or equal than target size, padding will be not processed. <code>fill</code> (int or list) If a single int is provided this is used to fill pixels with constant value. If a list of length 3, it is used to fill R, G, B channels respectively. Pad example - 1 <pre><code>augmentation:\n  train:\n    -\n      name: pad\n      size: 512\n      fill: 0\n</code></pre> Pad example - 2 <pre><code>augmentation:\n  train:\n    -\n      name: pad\n      size: [512, 512]\n      fill: 0\n</code></pre>"},{"location":"components/augmentation/transforms/#posetopdownaffine","title":"PoseTopDownAffine","text":"<p>Apply affine transform based on given bounding box. This augmentation is based on the RandomBBoxTransform and TopDownAffine in mmpose library.</p> Field  Description <code>name</code> (str) Name must be \"pad\" to use <code>Pad</code> transform. <code>scale</code> (list) Randomly adjust box scale in range of [<code>scale[0]</code>, <code>scale[1]</code>] <code>scale_prob</code> (float) The probability of applying scaling. If set to <code>1.0</code>, scale of box always randomly adjusted. <code>translate</code> (float) Randomly adjust the offset of the box by adding translate factor * box size. The translate factor is random value in range of [<code>0</code>, <code>translate</code>]. <code>translate_prob</code> (float) The probability of applying translate. If set to <code>1.0</code>, offset of box always randomly adjusted. <code>rotation</code> (int) Random rotation range of affine transform. The random value determined in [<code>-rotation</code>, <code>rotation</code>]. This rotation angle is degree. <code>rotation_prob</code> (float) The probability of applying rotation. If set to <code>1.0</code>, affine transform matrix always contains rotation value. PoseTopDownAffine example <pre><code>augmentation:\n  train:\n    - \n      name: posetopdownaffine\n      scale: [0.75, 1.25]\n      scale_prob: 1.\n      translate: 0.1\n      translate_prob: 1.\n      rotation: 60\n      rotation_prob: 1.\n      size: [*img_size, *img_size]\n</code></pre>"},{"location":"components/augmentation/transforms/#randomcrop","title":"RandomCrop","text":"<p>Crop the given image at a random location. This augmentation follows the RandomCrop in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomcrop\" to use <code>RandomCrop</code> transform. <code>size</code> (int or list) Desired output size of the crop. If size is an int, a square crop (size, size) is made. If provided a list of length 1, it will be interpreted as (size[0], size[0]). If a list of length 2 is provided, a square crop (size[0], size[1]) is made. RandomCrop example <pre><code>augmentation:\n  train:\n    - \n      name: randomcrop\n      size: 256\n</code></pre>"},{"location":"components/augmentation/transforms/#rancomerasing","title":"RancomErasing","text":"<p>Erase random area of given image. This augmentation follows the RandomErasing in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomerasing\" to use <code>RancomErasing</code> transform. <code>p</code> (float) The probability of applying random erasing. If <code>1.0</code>, it always applies. <code>scale</code> (list) Range of proportion of erased area against input image. <code>ratio</code> (list) Range of aspect ratio of erased area. <code>value</code> (int, optional) Erasing value. If <code>None</code>, erase image with random noise. <code>inplace</code> (bool) Whether to operate as inplace. RandomErasing example <pre><code>augmentation:\n  train:\n    - \n      name: randomerasing\n      p: 0.5\n      scale: [0.02, 0.33]\n      ratio: [0.3, 3.3]\n      value: 0\n      inplace: False\n</code></pre>"},{"location":"components/augmentation/transforms/#randomhorizontalflip","title":"RandomHorizontalFlip","text":"<p>Horizontally flip the given image randomly with a given probability. This augmentation follows the RandomHorizontalFlip in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomhorizontalflip\" to use <code>RandomHorizontalFlip</code> transform. <code>p</code> (float) the probability of applying horizontal flip. If <code>1.0</code>, it always applies the flip. RandomHorizontalFlip example <pre><code>augmentation:\n  train:\n    - \n      name: randomhorizontalflip\n      p: 0.5\n</code></pre>"},{"location":"components/augmentation/transforms/#randomresize","title":"RandomResize","text":"<p>RandomResize transforms the input image to a random size within a specified range. This random size range is determined by [<code>base_size[0]</code> - <code>stride</code> * <code>v</code>, <code>base_size[1]</code> + <code>stride</code> * <code>v</code>] with <code>stride</code> interval, where the value <code>v</code> is an integer within the range of [<code>-random_range</code>, <code>random_range</code>]. E.g. If <code>base_size = [256, 256]</code>, <code>stride = 32</code>, <code>random_range = 2</code>, possible output image sizes are <code>[[192, 192], [224, 224], [256, 256], [288, 288], [320, 320]]</code>.</p> <p>Since applying random resize to every image arises the difficulty of a batch handling, random resize should be applied on a per-batch basis. However, due to current implementation constraints, it's challenging to apply randomness at the batch level, so a single random size is determined per dataloader worker. The size managed by each worker changes to at the start of each epoch. Therefore, to fully benefit from RandomResize, the number of workers set by <code>environment.num_workers</code> needs to be sufficiently large.</p> Field  Description <code>name</code> (str) Name must be \"randomresize\" to use <code>RandomResize</code> transform. <code>base_size</code> (list) The base size of the output image after random resizing. The output size is determined based on <code>base_size</code>. <code>stride</code> (int) The interval at which the size variation occurs. <code>random_range</code> (int) The range for random size variation. The final size is determined in range of [<code>base_size[0]</code> - <code>stride</code> * <code>v</code>, <code>base_size[1]</code> + <code>stride</code> * <code>v</code>] with <code>stride</code> interval, where <code>v</code> is an integer within the range of [<code>-random_range</code>, <code>random_range</code>]. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. RandomResize <pre><code>augmentation:\n  train:\n    - \n      name: randomresize\n      base_size: [256, 256]\n      stride: 32\n      random_range: 4\n      interpolation: 'bilinear'\n</code></pre>"},{"location":"components/augmentation/transforms/#randomresizedcrop","title":"RandomResizedCrop","text":"<p>Crop a random portion of image with different aspect of ratio in width and height, and resize it to a given size. This augmentation follows the RandomResizedCrop in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomresizedcrop\" to use <code>RandomResizedCrop</code> transform. <code>size</code> (int or list) Desired output size of the crop. If size is an int, a square crop (<code>size</code>, <code>size</code>) is made. If provided a list of length 1, it will be interpreted as (<code>size[0]</code>, <code>size[0]</code>). If a list of length 2 is provided, a crop with size (<code>size[0]</code>, <code>size[1]</code>) is made. <code>scale</code> (float or list) Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image. <code>ratio</code> (float or list) lower and upper bounds for the random aspect ratio of the crop, before resizing. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. RandomResizedCrop <pre><code>augmentation:\n  train:\n    - \n      name: randomresizedcrop\n      size: 256\n      scale: [0.08, 1.0]\n      ratio: [0.75, 1.33]\n      interpolation: 'bilinear'\n</code></pre>"},{"location":"components/augmentation/transforms/#randomverticalflip","title":"RandomVerticalFlip","text":"<p>Vertically flip the given image randomly with a given probability. This augmentation follows the RandomVerticalFlip in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomverticalflip\" to use <code>RandomVerticalFlip</code> transform. <code>p</code> (float) the probability of applying vertical flip. If <code>1.0</code>, it always applies the flip. RandomVerticalFlip example <pre><code>augmentation:\n  train:\n    - \n      name: randomverticalflip\n      p: 0.5\n</code></pre>"},{"location":"components/augmentation/transforms/#resize","title":"Resize","text":"<p>Naively resize the input image to the given size. This augmentation follows the Resize in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"resize\" to use <code>Resize</code> transform. <code>size</code> (int or list) Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller or larger edge of the image will be matched to this number and keep aspect ratio. Determining match to smaller or larger edge is determined by <code>resize_criteria</code>. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. <code>max_size</code> (int, optional) The maximum allowed for the longer edge of the resized image: if the longer edge of the image exceeds <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller edge may be shorter than <code>size</code>. This is only supported if <code>size</code> is an int. <code>resize_criteria</code> (str, optional) This field only used when <code>size</code> is int. This determines which side (shorter or longer) to match with <code>size</code>, and only can have 'short' or 'long' or <code>None</code>. i.e, if <code>resize_criteria</code> is 'short' and height &gt; width, then image will be rescaled to (size * height / width, size). Resize example - 1 <pre><code>augmentation:\n  train:\n    - \n      name: resize\n      size: [256, 256]\n      interpolation: 'bilinear'\n      max_size: ~\n      resize_criteria: ~\n</code></pre> Resize example - 2 <pre><code>augmentation:\n  train:\n    - \n      name: resize\n      size: 256\n      interpolation: 'bilinear'\n      max_size: ~\n      resize_criteria: long\n</code></pre>"},{"location":"components/augmentation/transforms/#trivialaugmentwide","title":"TrivialAugmentWide","text":"<p>TrivialAugment based on TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. This augmentation follows the TrivialAugmentWide in the torchvision library. Currently, this transform function does not support segmentation and detection data.</p> Field  Description <code>name</code> (str) Name must be \"trivialaugmentwide\" to use <code>TrivialAugmentWide</code> transform. <code>num_magnitude_bins</code> (int) The number of different magnitude values. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. <code>fill</code> (list or int, optional) Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. TrivialAugmentWide example <pre><code>augmentation:\n  train:\n    - \n      name: trivialaugmentwide\n      num_magnitude_bins: 31\n      interpolation: 'bilinear'\n      fill: ~\n</code></pre>"},{"location":"components/model/losses/","title":"Losses","text":"<p>Loss modules are very important in the training of neural networks, because as they guide and shape the learning process by minimizing the loss. They can affect the speed of convergence during training and the overall robustness of the model. </p> <p>However, loss functions can vary depending on the task, especially in tasks like detection and segmentation, where specialized loss functions are required. Therefore, NetsPresso Trainer provides a predefined variety of loss modules, designed for flexible use across different tasks. Users can seamlessly apply the appropriate loss function to their desired task through simple configuration settings.</p>"},{"location":"components/model/losses/#supporting-loss-modules","title":"Supporting loss modules","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, hence most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/model/losses/#crossentropyloss","title":"CrossEntropyLoss","text":"<p>Cross entropy loss. This loss follows the CrossEntropyLoss in torch library.</p> Field  Description <code>criterion</code> (str) Criterion must be \"cross_entropy\" to use <code>CrossEntropyLoss</code>. <code>label_smoothing</code> (float) Specifies the amount of smoothing when computing the loss, where 0.0 means no smoothing. <code>weight</code> (float) Weight for this cross entropy loss. Cross entropy loss example <pre><code>model:\n  losses:\n    - criterion: cross_entropy\n      label_smoothing: 0.1\n      weight: ~\n</code></pre>"},{"location":"components/model/losses/#sigmoidfocalloss","title":"SigmoidFocalLoss","text":"<p>Focal loss based on Focal loss for dense object detections. This loss follows the sigmoid_focal_loss in the torch library.</p> Field  Description <code>criterion</code> (str) Criterion must be \"focal_loss\" to use <code>SigmoidFocalLoss</code>. <code>alpha</code> (float) Balancing parameter alpha for focal loss. <code>gamma</code> (float) Focusing parameter gamma for focal loss. <code>weight</code> (float) Weight for this focal loss. Focal loss example <pre><code>model:\n  losses:\n    - criterion: focal_loss\n      alpha: 0.25\n      gamma: 2.0\n      weight: ~\n</code></pre>"},{"location":"components/model/losses/#yoloxloss","title":"YOLOXLoss","text":"<p>Loss module for AnchorFreeDecoupledHead. This loss follows the YOLOX implementation.</p> Field  Description <code>criterion</code> (str) Criterion must be \"yolox_loss\" to use <code>YOLOXLoss</code>. <code>weight</code> (float) Weight for this YOLOX loss. <code>l1_activate_epoch</code> (int) Activate l1 loss at <code>l1_activate_epoch</code> epoch. YOLOX loss example <pre><code>model:\n  losses:\n    - criterion: yolox_loss\n      weight: ~\n      l1_activate_epoch: 1\n</code></pre>"},{"location":"components/model/losses/#retinanetloss","title":"RetinaNetLoss","text":"<p>Loss module for AnchorDecoupledHead. This loss follows torchvision implementation, it contains classification loss via focal loss and box regression loss via L1 loss.</p> Field  Description <code>criterion</code> (str) Criterion must be \"retinanet_loss\" to use <code>RetinaNetLoss</code>. <code>weight</code> (float) Weight for this RetinaNet loss. RetinaNetLoss loss example <pre><code>model:\n  losses:\n    - criterion: retinanet_loss\n      weight: ~\n</code></pre>"},{"location":"components/model/losses/#pidnetloss","title":"PIDNetLoss","text":"<p>Loss module for PIDNet. This loss follows official implementation repository.</p> Field  Description <code>criterion</code> (str) Criterion must be \"pidnet_loss\" to use <code>PIDNetLoss</code>. <code>weight</code> (float) Weight for this PIDNet loss. <code>ignore_index</code> (int) A target value that is ignored and does not contribute to the input gradient. PIDNetLoss loss example <pre><code>model:\n  losses:\n    - criterion: pidnet_loss\n      weight: ~\n      ignore_index: 255\n</code></pre>"},{"location":"components/model/overview/","title":"Model - Overview","text":"<p>Netspresso Trainer provides a variety of backbones and heads, allowing flexible combinations. Users can choose appropriate backbones and heads based on their dataset and task requirements. The models can be optimized for on-device environments using Netspresso's compression and converting services.</p> <p>We provide a configuration format which can easily construct backbones and heads to meet user requirements. As composed in the example of the ResNet50 model below, backbones and heads are structured as separate fields, and then connected. Also, Users can freely choose suitable loss modules suitable for the head and task. The range of supported models and the detailed configuration definitions for each model are extensively described in the separated Models page.</p> <pre><code>model:\n  task: classification\n  name: resnet50\n  checkpoint:\n    use_pretrained: True\n    load_head: False\n    path: ~\n    fx_model_path: ~\n    optimizer_path: ~\n  freeze_backbone: False\n  architecture:\n    full: ...\n    backbone: ...\n    neck: ...\n    head: ...\n  losses:\n    - criterion: cross_entropy\n      label_smoothing: 0.1\n      weight: ~\n</code></pre>"},{"location":"components/model/overview/#retraining-the-model-from-netspresso","title":"Retraining the model from NetsPresso","text":"<p>If you have compressed model from NetsPresso, then it's time to retrain your model to get the best performance. Netspresso Trainer uses the same configuration format for retraining torch.fx GraphModule. This can be executed by specifying the path to the torch.fx model in the <code>fx_model_checkpoint</code> field. Since the torch.fx model file contains the complete model definition, fields like <code>architecture</code> become unnecessary, can be ignored.</p> <pre><code>model:\n  task: classification\n  name: resnet50\n  checkpoint:\n    use_pretrained: # This field will be ignored since fx_model_path is activated\n    load_head: # This field will be ignored since fx_model_path is activated\n    path: # This field will be ignored since fx_model_path is activated\n    fx_model_path: ./path_to_your_fx_model.pt\n    optimizer_path: # This field will be ignored since fx_model_path is activated\n  freeze_backbone: False\n  architecture: # This field will be ignored since fx_model_path is activated\n  losses:\n    - criterion: cross_entropy\n      label_smoothing: 0.1\n      weight: ~\n</code></pre>"},{"location":"components/model/overview/#field-list","title":"Field list","text":"Field  Description <code>model.task</code> (str) We support \"classification\", \"segmentation\", and \"detection\" now. <code>model.name</code> (str) A nickname to identify the model. <code>model.checkpoint.use_pretrained</code> (bool) Whether to use the pretrained checkpoint. At first time, you will download and save the pretrained checkpoint. <code>model.checkpoint.load_head</code> (bool) Whether to use the pretrained checkpoint for <code>head</code> module. <code>model.checkpoint.path</code> (str) Checkpoint path to resume training. If <code>None</code> and <code>use_pretrained</code> is <code>False</code>, you can train you model from scratch. <code>model.checkpoint.optimizer_path</code> (str) Optimizer checkpoint path for resuming training. <code>model.checkpoint.fx_model_path</code> (str) Model path for fx model retraining. If you have to train the model from NP Compressor, you have to fill your compressed model path at this field. If <code>fx_model_path</code> is filled, <code>use_pretrained</code>, <code>load_head</code>, <code>path</code>, <code>optimizer_path</code>, and <code>freeze_backbone</code> are ignored. <code>model.freeze_backbone</code> (bool) Whether to freeze backbone in training. <code>model.architecture</code> (dict) Detailed configuration of the model architecture. Please see Model page to find NetsPresso supporting models. <code>model.losses</code> (list) List of losses that model to learn. Please see Losses page to find NetsPresso supporting loss modules."},{"location":"components/training/ema/","title":"EMA (Exponential Moving Average)","text":"<p>In many cases, providing a model with averaged parameters brings performance benefits. The Exponential Moving Average (EMA) model is updated after each batch training step according to the following:</p> <pre><code>ema_param = decay * ema_param + (1. - decay) * training_model_param\n</code></pre> <p>If EMA is enabled, both validation and model saving are processed with EMA model. Note that after the validation phase, the training model parameters are reverted back to the non-averaged model.</p>"},{"location":"components/training/ema/#ema-decay-schedulers","title":"EMA decay schedulers","text":"<p>It is often benefits to start with a smaller decay value at the beginning of training and gradually use higher values as progresses. To this, we support some decay scheduling methods.</p>"},{"location":"components/training/ema/#constant-decay","title":"Constant decay","text":"<p>Constant decay keeps the decay value unchanged throughout the entire training process.</p> Field  Description <code>training.epochs</code> (str) Name must be \"constant_decay\" to use constant decay. <code>training.decay</code> (float) The decay rate for EMA. Its range must be in [0, 1.0]. If <code>None</code>. Constant decay example <pre><code>training:\n  ema:\n    name: constant_decay\n    decay: 0.9999\n</code></pre>"},{"location":"components/training/ema/#exponential-decay","title":"Exponential decay","text":"<p>Exponential decay increases the decay value exponentially with the number of updates as following:</p> <pre><code>applied_decay = decay * (1 - math.exp(-counter / beta)\n</code></pre> <p><code>decay</code> and <code>beta</code> from configuration determine the maximum value of decay and the speed of convergence, respectively. The <code>counter</code> starts at 0 and increments by 1 with each update.</p> Field  Description <code>training.epochs</code> (str) Name must be \"exp_decay\" to use constant decay. <code>training.decay</code> (float) The decay rate for EMA. For exponential decay, this means maximum decay value. Its range must be in [0, 1.0]. <code>training.beta</code> (float) Determines the speed of convergence of decay to maximum value. Exponential decay example <pre><code>training:\n  ema:\n    name: exp_decay\n    decay: 0.9999\n    beta: 100\n</code></pre>"},{"location":"components/training/optimizers/","title":"Optimizers","text":"<p>NetsPresso Trainer uses the optimizers implemented in PyTorch as is. By selecting an optimizer that suits your training recipe, you can configure the optimal training. If you are unsure which optimizer to use, we recommend reading the blog post from towardsdatascience.</p>"},{"location":"components/training/optimizers/#supporting-optimizers","title":"Supporting optimizers","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/training/optimizers/#adadelta","title":"Adadelta","text":"<p>This optimizer follows the Adadelta in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adadelta\" to use <code>Adadelta</code> optimizer. <code>lr</code> (float) Coefficient that scales delta before it is applied to the parameters. <code>rho</code> (float) Coefficient used for computing a running average of squared gradients <code>weight_decay</code> (float) weight decay (L2 penalty). Adadelta example <pre><code>training:\n  optimizer:\n    name: adadelta\n    lr: 1.0\n    rho: 0.9\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adagrad","title":"Adagrad","text":"<p>This optimizer follows the Adagrad in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adagrad\" to use <code>Adagrad</code> optimizer. <code>lr</code> (float) Learning rate. <code>lr_decay</code> (float) Learning rate decay. <code>weight_decay</code> (float) weight decay (L2 penalty). Adagrad example <pre><code>training:\n  optimizer:\n    name: adagrad\n    lr: 1e-2\n    lr_decay: 0.\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adam","title":"Adam","text":"<p>This optimizer follows the Adam (<code>adam</code>) in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adam\" to use <code>Adam</code> optimizer. <code>lr</code> (float) Learning rate. <code>betas</code> (float) Coefficients used for computing running averages of gradient and its square. <code>weight_decay</code> (float) weight decay (L2 penalty). Adam example <pre><code>training:\n  optimizer:\n    name: adam\n    lr: 1e-3\n    betas: [0.9, 0.999]\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adamax","title":"Adamax","text":"<p>This optimizer follows the Adamax in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adamax\" to use <code>Adamax</code> optimizer. <code>lr</code> (float) Learning rate. <code>betas</code> (float) Coefficients used for computing running averages of gradient and its square. <code>weight_decay</code> (float) weight decay (L2 penalty). Adamax example <pre><code>training:\n  optimizer:\n    name: adamax\n    lr: 2e-3\n    betas: [0.9, 0.999]\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adamw","title":"AdamW","text":"<p>This optimizer follows the AdamW in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adamw\" to use <code>AdamW</code> optimizer. <code>lr</code> (float) Learning rate. <code>betas</code> (list[float]) Coefficients used for computing running averages of gradient and its square. <code>weight_decay</code> (float) weight decay (L2 penalty). AdamW example <pre><code>training:\n  optimizer:\n    name: adamw\n    lr: 1e-3\n    betas: [0.9, 0.999]\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#rmsprop","title":"RMSprop","text":"<p>This optimizer follows the RMSprop in torch library.</p> Field  Description <code>name</code> (str) Name must be \"rmsprop\" to use <code>RMSprop</code> optimizer. <code>lr</code> (float) Learning rate. <code>alpha</code> (float) Smoothing constant. <code>momentum</code> (float) Momentum factor. <code>weight_decay</code> (float) weight decay (L2 penalty). <code>eps</code> (float) Term added to the denominator to improve numerical stability. RMSprop example <pre><code>training:\n  optimizer:\n    name: rmsprop\n    lr: 1e-2\n    alpha: 0.99\n    momentum: 0.\n    weight_decay: 0.\n    eps: 1e-8\n</code></pre>"},{"location":"components/training/optimizers/#sgd","title":"SGD","text":"<p>This optimizer follows the SGD in torch library.</p> Field  Description <code>name</code> (str) Name must be \"sgd\" to use <code>SGD</code> optimizer. <code>lr</code> (float) Learning rate. <code>momentum</code> (float) Momentum factor. <code>weight_decay</code> (float) weight decay (L2 penalty). <code>nesterov</code> (bool) Enables Nesterov momentum. SGD example <pre><code>training:\n  optimizer:\n    name: sgd\n    lr: 1e-2\n    momentum: 0.\n    weight_decay: 0.\n    nesterov: false\n</code></pre>"},{"location":"components/training/overview/","title":"Overview","text":"<p>In training, the training recipe is just as important as the model architecture. Even if you have a good model architecture, the performance on the same data and model combination can vary greatly depending on your training recipe. NetsPresso Trainer not only introduces models optimized for edge devices, but also provides the ability to change training configurations to train these models with various data. The optimal training recipe will vary depending on the data you want to train. Use the options provided by NetsPresso Trainer to find the optimal training recipe for your data.</p> <p>Users can adjust epochs, the desired optimizer and scheduler as a following example.</p> <pre><code>training:\n  epochs: 300\n  ema:\n    name: constant_decay\n    decay: 0.9999\n  optimizer:\n    name: adamw\n    lr: 6e-5\n    betas: [0.9, 0.999]\n    weight_decay: 0.0005\n  scheduler:\n    name: cosine_no_sgdr\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 0.\n</code></pre>"},{"location":"components/training/overview/#field-list","title":"Field list","text":"Field  Description <code>training.epochs</code> (int) The total number of epoch for training the model <code>training.ema</code> (dict, optional) The configuration of EMA. Please refer to the EMA page for more details. If <code>None</code>, EMA is not applied. <code>training.optimizer</code> (dict) The configuration of optimizer. Please refer to the list of supporting optimizer for more details. <code>training.scheduler</code> (dict) The configuration of learning rate scheduler. Please refer to the list of supporting scheduler for more details."},{"location":"components/training/schedulers/","title":"Schedulers","text":"<p>NetsPresso Trainer supports various learning rate schedulers based on PyTorch. In particular, learning rate warm-up is supported for frequently used schedulers, and learning rate restart is supported for some schedulers, such as cosine annealing. NetsPresso Trainer updates the learning rate at the end of epoch, not the end of step, so users will set the scheduler with epoch-level counts.</p>"},{"location":"components/training/schedulers/#supporting-schedulers","title":"Supporting schedulers","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/training/schedulers/#step","title":"Step","text":"<p>This scheduler follows the StepLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"step\" to use <code>StepLR</code> scheduler. <code>iters_per_phase</code> (int) Epoch period of learning rate decay. <code>gamma</code> (float) Multiplicative factor of learning rate decay. <code>end_epoch</code> (int) End epoch of this scheduler. Remained epochs will be trained with fixed learning rate. Step example <pre><code>training:\n  scheduler:\n    name: step\n    iters_per_phase: 1\n    gamma: 0.1\n    end_epoch: 80\n</code></pre>"},{"location":"components/training/schedulers/#polynomial-with-warmup","title":"Polynomial with warmup","text":"<p>This scheduler follows the PolynomialLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"poly\" to use <code>PolynomialLRWithWarmUp</code> scheduler. <code>warmup_epochs</code> (int) The number of steps that the scheduler finishes to warmup the learning rate. <code>warmup_bias_lr</code> (float) Starting learning rate for warmup period. <code>min_lr</code> (float) Minimum learning rate. <code>power</code> (float) The power of the polynomial. <code>end_epoch</code> (int) End epoch of this scheduler. At the <code>end_epoch</code>, learning rate will be <code>min_lr</code>, and remained epochs trained with fixed learning rate. Polynomial with warmup example <pre><code>training:\n  scheduler:\n    name: poly\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 1e-6\n    power: 1.0\n    end_epoch: 80\n</code></pre>"},{"location":"components/training/schedulers/#cosine-annealing-with-warmup","title":"Cosine annealing with warmup","text":"<p>This scheduler follows the CosineAnnealingLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"cosine_no_sgdr\" to use <code>CosineAnnealingLRWithCustomWarmUp</code> scheduler. <code>warmup_epochs</code> (int) The number of steps that the scheduler finishes to warmup the learning rate. <code>warmup_bias_lr</code> (float) Starting learning rate for warmup period. <code>min_lr</code> (float) Minimum learning rate. <code>end_epoch</code> (int) End epoch of this scheduler. At the <code>end_epoch</code>, learning rate will be <code>min_lr</code>, and remained epochs trained with fixed learning rate. Cosine annealing with warmup example <pre><code>training:\n  scheduler:\n    name: cosine_no_sgdr\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 1e-6\n    end_epoch: 80\n</code></pre>"},{"location":"components/training/schedulers/#cosine-annealing-warm-restarts-with-warmup","title":"Cosine annealing warm restarts with warmup","text":"<p>This scheduler follows the CosineAnnealingWarmRestarts in torch library.</p> Field  Description <code>name</code> (str) Name must be \"cosine\" to use <code>CosineAnnealingWarmRestartsWithCustomWarmUp</code> scheduler. <code>warmup_epochs</code> (int) The number of steps that the scheduler finishes to warmup the learning rate. <code>warmup_bias_lr</code> (float) Starting learning rate for warmup period. <code>min_lr</code> (float) Minimum learning rate. <code>iters_per_phase</code> (float) Epoch period for the learning rate restart. Cosine annealing warm restart with warmup example <pre><code>training:\n  scheduler:\n    name: cosine\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 1e-6\n    iters_per_phase: 10\n</code></pre>"},{"location":"components/training/schedulers/#gradio-demo-for-simulating-the-learning-rate-scheduler","title":"Gradio demo for simulating the learning rate scheduler","text":"<p>In many training feature repositories, it is recommended to perform the entire training pipeline and check the log to see how the learning rate scheduler works. NetsPresso Trainer supports learning rate schedule simulation to allow users to easily understand the learning rate scheduler for their configured training recipe. By copying and pasting the training configuration into the simulator, users can see how the learning rate changes every epoch.</p> <p> This simulation is not supported for some schedulers which adjust the learning rate dynamically with training results.</p>"},{"location":"components/training/schedulers/#running-on-your-environment","title":"Running on your environment","text":"<p>Please run the gradio demo with following command:</p> <pre><code>bash scripts/run_simulator_lr_scheduler.sh\n</code></pre>"},{"location":"getting_started/simple_use/","title":"Simple use","text":""},{"location":"getting_started/simple_use/#training","title":"Training","text":"<p>Write your training script in <code>train.py</code> like:</p> <pre><code>from netspresso_trainer import train_cli\n\nif __name__ == '__main__':\n    logging_dir = train_cli()\n    print(f\"Training results are saved at: {logging_dir}\")\n</code></pre> <p>Then, train your model with your own configuraiton:</p> <pre><code>python train.py\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --training config/training.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre> <p>Or you can start NetsPresso Trainer by just executing console script which has same feature.</p> <pre><code>netspresso-train\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --training config/training.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre> <p>Please refer to <code>scripts/example_train.sh</code>.</p> <p>NetsPresso Trainer is compatible with NetsPresso service. We provide NetsPresso Trainer tutorial that contains whole procedure from model train to model compression and benchmark. Please refer to our colab tutorial.</p>"},{"location":"getting_started/simple_use/#evaluation","title":"Evaluation","text":"<p>Write your evaluation script in <code>evaluation.py</code> like:</p> <pre><code>from netspresso_trainer import evaluation_cli\n\nif __name__ == '__main__':\n    logging_dir = evaluation_cli()\n\n    print(f\"Evaluation results are saved at: {logging_dir}\")\n</code></pre> <p>Then, evaluate your model with your own configuraiton:</p> <pre><code>python evaluation.py\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre>"},{"location":"getting_started/simple_use/#inference","title":"Inference","text":"<p>Write your inference script in <code>inference.py</code> like:</p> <pre><code>from netspresso_trainer import inference_cli\n\nif __name__ == '__main__':\n    logging_dir = inference_cli()\n\n    print(f\"Inference results are saved at: {logging_dir}\")\n</code></pre> <p>Then, inference your dataset:</p> <pre><code>python inference.py\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/","title":"Data preparation (Hugging Face)","text":"<p>The Hugging Face datasets offers a vast array of datasets to support various tasks, making them readily accessible through a user-friendly API. Provided datasets by Hugging Face datasets are typically structured into <code>training</code>, <code>validation</code>, and <code>testing</code> sets. This structure allows NetsPresso Trainer to utilize various datasets with yaml configuration.</p> <p>To explore official Hugging Face datasets catalogue, please refer to the hugging Face datasets page.</p>"},{"location":"getting_started/dataset_preparation/huggingface/#hugging-face-datasets-install","title":"Hugging Face datasets install","text":"<p>First, you must install the hugging Face datasets library.</p> <pre><code>pip install -r requirements-optional.txt\n\nor\n\npip install datasets\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#find-dataset-repository","title":"Find dataset repository","text":"<p>We use CIFAR100 dataset as an example. Thus, <code>format</code> field in data configuration is filled as huggingface, and <code>metadata.repo</code> is filled as <code>cifar100</code>.</p> <pre><code>data:\n  name: cifar100\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: cifar100\n    subset: ~\n    features:\n      image: ~\n      label: ~\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#agreement-the-conditions-to-access-the-datasets","title":"Agreement the conditions to access the datasets","text":"<p>Some datasets are publicly available, but may require a agreement to be used. For instance, to use ImageNet1K in Hugging Face datasets, you have to log in to Hugging Face homepage and accpet the conditions.</p> <p>Make sure that you agreed to the conditions on Hugging Face website, and log in to <code>huggingface-cli</code> before you start.</p> <pre><code>huggingface-cli login\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#set-subset","title":"Set subset","text":"<p>Some datasets have multiple subsets in their dataset hierarchy. They have to be specified in <code>subset</code> field.</p> <p>If there is no subset in the dataset, you can leave <code>subset</code> field as null.</p> <pre><code>data:\n  name: cifar100\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: cifar100\n    subset: ~ # You have to fill this field if there is subset in the dataset you trying to use\n    features:\n      image: ~\n      label: ~\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#set-features","title":"Set features","text":"<p>You should check features of dataset in Hugging Face homepage. If you see CIFAR100, there are three features in the dataset which are <code>img</code>, <code>fine_label</code>, <code>coarse_label</code>. In this dataset, the image data is denoted by <code>img</code> and the labels for the 100 classes are represented by <code>fine_label</code>. Given this structure, the data configuration should be filled out as below.</p> <pre><code>data:\n  name: cifar100\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: cifar100\n    subset: ~ # You should fill this field if there is subset in the dataset you trying to use\n    features:\n      image: img\n      label: fine_label\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#run-netspresso-trainer","title":"Run NetsPresso Trainer","text":"<p>Now you can run NetsPresso Trainer with Hugging Face dataset!</p> <pre><code>python train.py --data your_huggingface_dataset_yaml_path.yaml ...\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/","title":"Data preparation (Local)","text":""},{"location":"getting_started/dataset_preparation/local/#local-custom-datasets","title":"Local custom datasets","text":"<p>If your dataset is ready in local storage, you can use them by following the instructions.</p>"},{"location":"getting_started/dataset_preparation/local/#organize-dataset","title":"Organize dataset","text":"<p>Create separate directories for images/labels and train/valid/test.</p> <pre><code>/my_dataset\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 valid\n\u2502   \u2514\u2500\u2500 test\n\u2514\u2500\u2500 labels\n</code></pre> <p>Place your images on proper path.</p> <pre><code>/my_dataset\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 valid\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u251c\u2500\u2500 img1.jpg\n\u2502       \u251c\u2500\u2500 img2.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n</code></pre> <p>Set labels on proper path.</p> <ul> <li>For image classification, you may need csv format label files.</li> <li>For semantic segmentation and object detection, organize your label files (could be masks or box annotations) in corresponding folders.</li> </ul> <pre><code>/my_dataset\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 valid\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u251c\u2500\u2500 img1.jpg\n\u2502       \u251c\u2500\u2500 img2.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u2514\u2500\u2500 train directory or file ...\n    \u251c\u2500\u2500 valid directory or file ...\n    \u2514\u2500\u2500 test directory or file ...\n</code></pre> <p>If you just run training, test split may not needed.</p> <p>If you just run evaluation or inference, train and valid split may not needed.</p>"},{"location":"getting_started/dataset_preparation/local/#set-configuration-file","title":"Set configuration file","text":"<p>Define the paths to your datasets in the configuration file to tell NetsPresso Trainer where to find the data. Finally, you can complete data configuration by adding some metadata like <code>id_mapping</code>. Here is example for classification:</p> <pre><code>data:\n  name: my_custom_dataset\n  task: classification # This could be other task\n  format: local\n  path:\n    root: ./my_dataset\n    train:\n      image: train/images\n      label: train/labels.csv\n    valid:\n      image: valid/images\n      label: valid/labels.csv\n    test:\n      image: test/images\n      label: test/labels.csv\n  id_mapping: [cat, dog, elephant]\n</code></pre> <p>For detailed definition of data configuration, please refer to components/data</p>"},{"location":"getting_started/dataset_preparation/local/#open-datasets","title":"Open datasets","text":"<p>If you are interested in using open datasets, follow the instructions below to seamlessly integrate them into the local custom datasets format.</p>"},{"location":"getting_started/dataset_preparation/local/#image-classification","title":"Image classification","text":""},{"location":"getting_started/dataset_preparation/local/#cifar100","title":"CIFAR100","text":"<p>Run <code>cifar100.py</code> python file with your dataset directory as an argument.</p> <p>CIFAR100 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/cifar100.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#imagenet1k","title":"ImageNet1K","text":"<p>ImageNet1K dataset cannot be automatically downloaded. You should download dataset from ImageNet website, and place downloaded files into <code>./data/download</code>.</p> <p>And, run <code>imagenet1k.py</code> python file with your dataset directorty and downloaded files path as arguments. After executing scripts, you can use pre-defined configuration.</p> <p>(<code>imagenet1k.py</code> needs scipy library which is in requirements-optional.txt)</p> <pre><code>python ./tools/open_dataset_tool/imagenet1k.py --dir ./data --train-images ./data/download/ILSVRC2012_img_train.tar --valid-images ./data/download/ILSVRC2012_img_val.tar --devkit ./data/download/ILSVRC2012_devkit_t12.tar.gz\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#semantic-segmentation","title":"Semantic segmentation","text":""},{"location":"getting_started/dataset_preparation/local/#pascalvoc-2012","title":"PascalVOC 2012","text":"<p>Run <code>voc2012_seg.py</code> python file with your dataset directory as an argument.</p> <p>PascalVOC 2012 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/voc2012_seg.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#object-detection","title":"Object detection","text":""},{"location":"getting_started/dataset_preparation/local/#coco-2017","title":"COCO 2017","text":"<p>Run <code>coco2017.py</code> python file with your dataset directory as an argument.</p> <p>COCO 2017 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/coco2017.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#pose-estimation","title":"Pose estimation","text":""},{"location":"getting_started/dataset_preparation/local/#wflw","title":"WFLW","text":"<p>Run <code>wflw.py</code> python file with your dataset directory as an argument.</p> <p>WFLW dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/wflw.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#run-netspresso-trainer","title":"Run NetsPresso Trainer","text":"<p>Now you can run NetsPresso Trainer with your local dataset!</p> <pre><code>python train.py --data your_huggingface_dataset_yaml_path.yaml ...\n</code></pre>"},{"location":"getting_started/installation/docker_installation/","title":"Setup with Docker","text":""},{"location":"getting_started/installation/docker_installation/#installation-with-docker","title":"Installation with docker","text":""},{"location":"getting_started/installation/docker_installation/#docker-with-docker-compose","title":"Docker with docker-compose","text":"<p>For the latest information, please check <code>docker-compose.yml</code></p> <pre><code># run command\nexport TAG=v$(cat src/netspresso_trainer/VERSION) &amp;&amp; \\\ndocker compose run --service-ports --name netspresso-trainer-dev netspresso-trainer bash\n</code></pre>"},{"location":"getting_started/installation/docker_installation/#docker-image-build","title":"Docker image build","text":"<p>If you run with <code>docker run</code> command, follow the image build and run command in the below:</p> <pre><code># build an image\ndocker build -t netspresso-trainer:v$(cat src/netspresso_trainer/VERSION) .\n</code></pre> <pre><code># docker run command\ndocker run -it --ipc=host\\\n  --gpus='\"device=0,1,2,3\"'\\\n  -v /PATH/TO/DATA:/DATA/PATH/IN/CONTAINER\\\n  -v /PATH/TO/CHECKPOINT:/CHECKPOINT/PATH/IN/CONTAINER\\\n  -p 50001:50001\\\n  -p 50002:50002\\\n  -p 50003:50003\\\n  --name netspresso-trainer-dev netspresso-trainer:v$(cat src/netspresso_trainer/VERSION)\n</code></pre>"},{"location":"getting_started/installation/installation/","title":"Installation (Stable)","text":""},{"location":"getting_started/installation/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python <code>3.8</code> | <code>3.9</code> | <code>3.10</code></li> <li>PyTorch <code>1.13.0</code> (recommended) (compatible with: <code>1.11.x</code> - <code>1.13.x</code>)</li> </ul>"},{"location":"getting_started/installation/installation/#install-with-pypi","title":"Install with pypi","text":"<pre><code>pip install netspresso_trainer\n</code></pre>"},{"location":"getting_started/installation/installation/#install-with-github","title":"Install with GitHub","text":"<p>To use pip install,</p> <pre><code>pip install git+https://github.com/Nota-NetsPresso/netspresso-trainer.git@master\n</code></pre> <p>To install with editable mode,</p> <pre><code>git clone -b master https://github.com/Nota-NetsPresso/netspresso-trainer.git\npip install -e netspresso-trainer\n</code></pre>"},{"location":"models/overview/","title":"Overview","text":"<p>This section describes the architecture configuration design of models. For details on NetsPresso Trainer's model configuration excluding the architecture, please refer to the model components page.</p> <p>NetsPresso Trainer prioritize model compression and device deployment, thus models fulfill the following criteria:</p> <ul> <li>Compatible with torch.fx converting.</li> <li>Can be compressed by pruning method provided in NetsPresso.</li> <li>Can be easily deployed at many edge devices.</li> </ul> <p>To provide a wide range of models that meet these conditions in diverse forms, we define and use four fields for model definition: full, backbone, neck, and head. This approach allows users to utilize backbones, necks, and heads in desired configurations. For models that cannot be segmented into these three modules, we provide them in a full models.</p> <pre><code>model:\n  architecture:\n    full: ~ # For full model which can't be separated to backbone, neck and head.\n    backbone: ~ # Model backbone configuration.\n    neck: ~ # Model neck configuration.\n    head: ~ # Model head configuration.\n</code></pre>"},{"location":"models/overview/#field-list","title":"Field list","text":"Field  Description <code>full</code> (dict) If the model does not distinctly separated to backbone, neck, and head, the model's details are defined under this field. If this field is not <code>None</code>, the <code>backbone</code>, <code>neck</code>, and <code>head</code> fields are ignored. <code>backbone</code> (dict) This field defines the model's backbone, applicable only when the <code>full</code> field is <code>None</code>. <code>neck</code> (dict) This field defines the model's neck, applicable only when the <code>full</code> field is <code>None</code>. This can be <code>None</code> anytime because the necessity of the neck module may vary depending on the task. <code>head</code> (dict) This field defines the model's head, applicable only when the <code>full</code> field is <code>None</code>."},{"location":"models/backbones/cspdarknet/","title":"CSPDarkNet","text":"<p>CSPDarkNet backbone based on YOLOX: Exceeding YOLO Series in 2021.</p> <p>CSPDarkNet is a modified model from Darknet53 by adopting the strategy of CSPNet. Therefore, the structure of the model is fixed, and neither the number of stages nor type of blocks can be changed. The size of the model is determined by two values, which define the feature dimensions within the model and the repetition of CSPLayers.</p>"},{"location":"models/backbones/cspdarknet/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/cspdarknet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"cspdarknet\" to use <code>CSPDarkNet</code> backbone. <code>params.dep_mul</code> (float) Multiplying factor determining the repetition count of <code>CSPLayer</code> in the backbone. <code>params.wid_mul</code> (float) Multiplying factor adjusting the input/output dimensions of convolutional layers throughout the backbone. <code>params.act_type</code> (str) Type of activation function for the model. Supporting activation functions are described in [here]."},{"location":"models/backbones/cspdarknet/#model-configuration-examples","title":"Model configuration examples","text":"CSPDarkNet-s <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        dep_mul: &amp;dep_mul 0.33\n        wid_mul: 0.5\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-m <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        dep_mul: &amp;dep_mul 0.67\n        wid_mul: 0.75\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-l <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        dep_mul: &amp;dep_mul 1.0\n        wid_mul: 1.0\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-x <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        dep_mul: &amp;dep_mul 1.33\n        wid_mul: 1.25\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre>"},{"location":"models/backbones/cspdarknet/#related-links","title":"Related links","text":"<ul> <li><code>Megvii-BaseDetection/YOLOX</code></li> </ul>"},{"location":"models/backbones/efficientformer/","title":"EfficientFormer","text":"<p>EfficientFormer backbone based on EfficientFormer: Vision Transformers at MobileNet Speed.</p> <p>EfficientFormer is designed following the design principle of MetaFormer, constructing its backbone by stacking MetaBlocks. 4D MetaBlocks are employed throughout the model, and 3D MetaBlocks are used at the end of the backbone to enhance the model's expression power. We provide configuration options to adjust the design settings including repetition values for 3D and 4D MetaBlocks.</p>"},{"location":"models/backbones/efficientformer/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/efficientformer/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"efficientformer\" to use EfficientFormer backbone. <code>params.num_attention_heads</code> (int) The number of heads in the multi-head attention of 3D MetaBlock. <code>params.attention_channels</code> (int) Dimension for attention of 3D MetaBlock. <code>params.attention_dropout_prob</code> (float) Dropout probability for attention of 3D MetaBlock. <code>params.attention_value_expansion_ratio</code> (int) Value dimension expansion ratio of 3D MetaBlock. <code>params.ffn_intermediate_ratio</code> (int) Dimension expansion ratio of MLP layer in 3D and 4D MetaBlock. <code>params.ffn_dropout_prob</code> (float) Dropout probability of MLP layer in 3D and 4D MetaBlock. <code>params.ffn_act_type</code> (str) Activation function of MLP layer in 3D and 4D MetaBlocks <code>params.vit_num</code> (int) The number of last 3D MetaBlock. <code>stage_params[n].num_blocks</code> (int) The number of 4D MetaBlock in the stage. <code>stage_params[n].channels</code> (int) Dimensions for 4D MetaBlock in the stage."},{"location":"models/backbones/efficientformer/#model-configuration-examples","title":"Model configuration examples","text":"EfficientFormer-L1 <pre><code>model:\n  architecture:\n    backbone:\n      name: efficientformer\n      params:\n        num_attention_heads: 8\n        attention_channels: 256  # attention_hidden_size_splitted * num_attention_heads\n        attention_dropout_prob: 0.\n        attention_value_expansion_ratio: 4\n        ffn_intermediate_ratio: 4\n        ffn_dropout_prob: 0.\n        ffn_act_type: 'gelu'\n        vit_num: 1\n      stage_params:\n        - \n          num_blocks: 3\n          channels: 48\n        - \n          num_blocks: 2\n          channels: 96\n        - \n          num_blocks: 6\n          channels: 224\n        - \n          num_blocks: 4\n          channels: 448\n</code></pre> EfficientFormer-L3 <pre><code>model:\n  architecture:\n    backbone:\n      name: efficientformer\n      params:\n        num_attention_heads: 8\n        attention_channels: 256  # attention_hidden_size_splitted * num_attention_heads\n        attention_dropout_prob: 0.\n        attention_value_expansion_ratio: 4\n        ffn_intermediate_ratio: 4\n        ffn_dropout_prob: 0.\n        ffn_act_type: 'gelu'\n        vit_num: 4\n      stage_params:\n        - \n          num_blocks: 4\n          channels: 64\n        - \n          num_blocks: 4\n          channels: 128\n        - \n          num_blocks: 12\n          channels: 320\n        - \n          num_blocks: 6\n          channels: 512\n</code></pre>"},{"location":"models/backbones/efficientformer/#related-links","title":"Related links","text":"<ul> <li><code>snap-research/EfficientFormer</code></li> </ul>"},{"location":"models/backbones/mixnet/","title":"MixNet","text":"<p>MixNet backbone based on MixConv: Mixed Depthwise Convolutional Kernels.</p> <p>Similar with MobileNetV3, we provide a configuration that allows users to define each MixDepthBlock in MixNet individually. You can specify in a list format the number and form of each MixDepthBlock for every stage. Using this, we pre-construct and provide MixNet family models.</p>"},{"location":"models/backbones/mixnet/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/mixnet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mixnet\" to use <code>MixNet</code> backbone. <code>params.stem_channels</code> (int) Output dimension of the first convolution layer. <code>params.wid_mul</code> (float) Ratio for adjusting the input/output dimensions for the entire model. <code>params.dep_mul</code> (float) Ratio for adjusting the <code>num_block</code> value for the entire model. <code>params.dropout_rate</code> (float) Dropout ratio applied to all <code>MixDepthBlock</code>. <code>stage_params[n].expansion_ratio</code> (list[int]) Determines the output dimension of the expansion phase in each <code>MixDepthBlock</code>. Expands the input dimensions by multiplying the <code>expansion_ratio</code>. <code>stage_params[n].out_channels</code> (list[int]) Output dimensions of each <code>MixDepthBlock</code>. <code>stage_params[n].num_blocks</code> (list[int]) Repetition count for each <code>MixDepthBlock</code>. <code>stage_params[n].kernel_sizes</code> (list[list[int]]) Various kernel sizes used within each <code>MixDepthBlock</code>. <code>stage_params[n].num_exp_groups</code> (list[int]) The number of convolution groups in the expansion phase of <code>MixDepthBlock</code>. <code>stage_params[n].num_poi_groups</code> (list[int]) The number of convolution groups in the final point-wise convolution of <code>MixDepthBlock</code>. <code>stage_params[n].stride</code> (list[int]) Stride values for each <code>MixDepthBlock</code>. <code>stage_params[n].act_type</code> (list[str]) Activation function for each <code>MixDepthBlock</code>. Supporting activation functions are described in [here] <code>stage_params[n].se_reduction_ratio</code> (list[int]) Reduction factor for calculating the output dimension of the squeeze-and-excitation block. If <code>None</code>, the squeeze-and-excitation block is not applied."},{"location":"models/backbones/mixnet/#model-configuration-examples","title":"Model configuration examples","text":"MixNet-s <pre><code>model:\n  architecture:\n    backbone:\n      name: mixnet\n      params:\n        stem_channels: 16\n        wid_mul: 1.0\n        dep_mul: 1.0\n        dropout_rate: 0.\n      stage_params: \n        -\n          expansion_ratio: [1, 6, 3]\n          out_channels: [16, 24, 24]\n          num_blocks: [1, 1, 1]\n          kernel_sizes: [[3], [3], [3]]\n          num_exp_groups: [1, 2, 2]\n          num_poi_groups: [1, 2, 2]\n          stride: [1, 2, 1]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          se_reduction_ratio: [~, ~, ~]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [40, 40]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7], [3, 5]]\n          num_exp_groups: [1, 2]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n        -\n          expansion_ratio: [6, 6, 6, 3]\n          out_channels: [80, 80, 120, 120]\n          num_blocks: [1, 2, 1, 2]\n          kernel_sizes: [[3, 5, 7], [3, 5], [3, 5, 7], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1, 2, 2]\n          num_poi_groups: [2, 2, 2, 2]\n          stride: [2, 1, 1, 1]\n          act_type: [\"swish\", \"swish\", \"swish\", \"swish\"]\n          se_reduction_ratio: [4, 4, 2, 2]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [200, 200]\n          num_blocks: [1, 2]\n          kernel_sizes: [[3, 5, 7, 9, 11], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n</code></pre> MixNet-m <pre><code>model:\n  architecture:\n    backbone:\n      name: mixnet\n      params:\n        stem_channels: 24\n        wid_mul: 1.0\n        dep_mul: 1.0\n        dropout_rate: 0.\n      stage_params: \n        -\n          expansion_ratio: [1, 6, 3]\n          out_channels: [24, 32, 32]\n          num_blocks: [1, 1, 1]\n          kernel_sizes: [[3], [3, 5, 7], [3]]\n          num_exp_groups: [1, 2, 2]\n          num_poi_groups: [1, 2, 2]\n          stride: [1, 2, 1]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          se_reduction_ratio: [~, ~, ~]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [40, 40]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5]]\n          num_exp_groups: [1, 2]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n        -\n          expansion_ratio: [6, 6, 6, 3]\n          out_channels: [80, 80, 120, 120]\n          num_blocks: [1, 3, 1, 3]\n          kernel_sizes: [[3, 5, 7], [3, 5, 7, 9], [3], [3, 5, 7, 9]]\n          num_exp_groups: [1, 2, 1, 2]\n          num_poi_groups: [1, 2, 1, 2]\n          stride: [2, 1, 1, 1]\n          act_type: [\"swish\", \"swish\", \"swish\", \"swish\"]\n          se_reduction_ratio: [4, 4, 2, 2]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [200, 200]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n</code></pre> MixNet-l <pre><code>model:\n  architecture:\n    backbone:\n      name: mixnet\n      params:\n        stem_channels: 24\n        wid_mul: 1.3\n        dep_mul: 1.0\n        dropout_rate: 0.\n      stage_params: \n        -\n          expansion_ratio: [1, 6, 3]\n          out_channels: [24, 32, 32]\n          num_blocks: [1, 1, 1]\n          kernel_sizes: [[3], [3, 5, 7], [3]]\n          num_exp_groups: [1, 2, 2]\n          num_poi_groups: [1, 2, 2]\n          stride: [1, 2, 1]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          se_reduction_ratio: [~, ~, ~]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [40, 40]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5]]\n          num_exp_groups: [1, 2]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n        -\n          expansion_ratio: [6, 6, 6, 3]\n          out_channels: [80, 80, 120, 120]\n          num_blocks: [1, 3, 1, 3]\n          kernel_sizes: [[3, 5, 7], [3, 5, 7, 9], [3], [3, 5, 7, 9]]\n          num_exp_groups: [1, 2, 1, 2]\n          num_poi_groups: [1, 2, 1, 2]\n          stride: [2, 1, 1, 1]\n          act_type: [\"swish\", \"swish\", \"swish\", \"swish\"]\n          se_reduction_ratio: [4, 4, 2, 2]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [200, 200]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n</code></pre>"},{"location":"models/backbones/mixnet/#related-links","title":"Related links","text":""},{"location":"models/backbones/mixtransformer/","title":"MixTransformer","text":"<p>MixTransformer backbone based on SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.</p> <p>We provide the MixTransformer encoder (MiT), the backbone of SegFormer, as a freely usable backbone module. Users have the flexibility to configure the transformer encoder for each stage, enabling MiT-b0 to MiT-b5.</p>"},{"location":"models/backbones/mixtransformer/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/mixtransformer/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mixtransformer\" to use <code>MixTransformer</code> backbone. <code>params.ffn_intermediate_expansion_ratio</code> (int) Expansion factor to compute intermediate dimension in feed-forward network. <code>params.ffn_act_type</code> (str) Activation function for feed-forward network in the transformer block. Supporting activation functions are described in [here]. <code>params.ffn_dropout_prob</code> (float) Dropout probability for feed-forward network in the transformer block. <code>params.attention_dropout_prob</code> (float) Dropout probability for attention in the transformer block. <code>stage_params[n].num_blocks</code> (int) The number of transformer blocks in the encoder. <code>stage_params[n].sequence_reduction_ratio</code> (int) Sequence reduction ratio for multi-head attention. <code>stage_params[n].encoder_chananels</code> (int) Dimension for the transformer block. <code>stage_params[n].embedding_patch_sizes</code> (int) Kernel size for convolution layer in overlapping patch embedding. <code>stage_params[n].embedding_strides</code> (int) stride value for convolution layer in overlapping patch embedding. <code>stage_params[n].num_attention_heads</code> (int) The number of heads in the multi-head attention."},{"location":"models/backbones/mixtransformer/#model-configuration-examples","title":"Model configuration examples","text":"MiT-b0 <pre><code>model:\n  architecture:\n    backbone:\n      name: mixtransformer\n      params:\n        ffn_intermediate_expansion_ratio: 4\n        ffn_act_type: \"gelu\"\n        ffn_dropout_prob: 0.0\n        attention_dropout_prob: 0.0\n      stage_params:\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 8\n          attention_chananels: 32\n          embedding_patch_sizes: 7\n          embedding_strides: 4\n          num_attention_heads: 1\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 4\n          attention_chananels: 64\n          embedding_patch_sizes: 3\n          embedding_strides: 2\n          num_attention_heads: 2\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 2\n          attention_chananels: 160\n          embedding_patch_sizes: 3\n          embedding_strides: 2\n          num_attention_heads: 5\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 1\n          attention_chananels: 256\n          embedding_patch_sizes: 3\n          embedding_strides: 2\n          num_attention_heads: 8\n</code></pre>"},{"location":"models/backbones/mixtransformer/#related-links","title":"Related links","text":"<ul> <li><code>huggingface/transformers</code></li> </ul>"},{"location":"models/backbones/mobilenetv3/","title":"MobileNetV3","text":"<p>MobileNetV3 backbone based on Searching for MobileNetV3.</p> <p>We provide a configuration that allows users to define each inverted residual block in MobileNetV3 individually. You can specify in a list format the number and form of each inverted residual block for every stage. Using this, we provide both MobileNetV3-small and MobileNetV3-large.</p>"},{"location":"models/backbones/mobilenetv3/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/mobilenetv3/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mobilenetv3\" to use <code>MobileNetV3</code> backbone. <code>stage_params[n].in_channels</code> (list[int]) Input dimensions for the inverted residual blocks in the stage. <code>stage_params[n].kernel_sizes</code> (list[int]) Convolution kernel sizes for the inverted residual blocks in the stage. <code>stage_params[n].expanded_channels</code> (list[int]) Expanded dimensions for the inverted residual blocks in the stage. <code>stage_params[n].out_channels</code> (list[int]) Output dimensions for the inverted residual blocks in the stage. <code>stage_params[n].use_se</code> (list[bool]) Flags that determine whether to use squeeze-and-excitation blocks for the inverted residual blocks in the stage. <code>stage_params[n].activation</code> (list[str]) Type of activation functions for the inverted residual blocks in the stage. Supporting activation functions are described in [here] <code>stage_params[n].stride</code> (list[int]) Stride values for the inverted residual blocks included in the stage. <code>stage_params[n].dilation</code> (list[int]) Dilation values for the inverted residual blocks in the stage."},{"location":"models/backbones/mobilenetv3/#model-configuration-examples","title":"Model configuration examples","text":"MobileNetV3-small <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv3\n      params: ~\n      stage_params:\n        -\n          in_channels: [16]\n          kernel_sizes: [3]\n          expanded_channels: [16]\n          out_channels: [16]\n          use_se: [True]\n          act_type: [\"relu\"]\n          stride: [2]\n        -\n          in_channels: [16, 24]\n          kernel_sizes: [3, 3]\n          expanded_channels: [72, 88]\n          out_channels: [24, 24]\n          use_se: [False, False]\n          act_type: [\"relu\", \"relu\"]\n          stride: [2, 1]\n        -\n          in_channels: [24, 40, 40, 40, 48]\n          kernel_sizes: [5, 5, 5, 5, 5]\n          expanded_channels: [96, 240, 240, 120, 144]\n          out_channels: [40, 40, 40, 48, 48]\n          use_se: [True, True, True, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1, 1, 1]\n        -\n          in_channels: [48, 96, 96]\n          kernel_sizes: [5, 5, 5]\n          expanded_channels: [288, 576, 576]\n          out_channels: [96, 96, 96]\n          use_se: [True, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1]\n</code></pre> MobileNetV3-large <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv3\n      params: ~\n      stage_params:\n        -\n          in_channels: [16, 16, 24]\n          kernel_sizes: [3, 3, 3]\n          expanded_channels: [16, 64, 72]\n          out_channels: [16, 24, 24]\n          use_se: [False, False, False]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          stride: [1, 2, 1]\n        - \n          in_channels: [24, 40, 40]\n          kernel_sizes: [5, 5, 5]\n          expanded_channels: [72, 120, 120]\n          out_channels: [40, 40, 40]\n          use_se: [True, True, True]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          stride: [2, 1, 1]\n        -\n          in_channels: [40, 80, 80, 80, 80, 112]\n          kernel_sizes: [3, 3, 3, 3, 3, 3]\n          expanded_channels: [240, 200, 184, 184, 480, 672]\n          out_channels: [80, 80, 80, 80, 112, 112]\n          use_se: [False, False, False, False, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1, 1, 1, 1]\n        -\n          in_channels: [112, 160, 160]\n          kernel_sizes: [5, 5, 5]\n          expanded_channels: [672, 960, 960]\n          out_channels: [160, 160, 160]\n          use_se: [True, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1]\n</code></pre>"},{"location":"models/backbones/mobilenetv3/#related-links","title":"Related links","text":"<ul> <li><code>pytorch/vision</code></li> </ul>"},{"location":"models/backbones/mobilevit/","title":"MobileViT","text":"<p>MobileViT backbone based on MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer.</p> <p>MobileViT was introduced by combining inverted residual blocks with transformer-based MobileViT blocks. In line with this, it is possible to select between inverted residual blocks (as mv2) and MobileViT models for each stage of the backbone with detailed configurations according to each block type.</p>"},{"location":"models/backbones/mobilevit/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/mobilevit/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mobilevit\" to use MobileViT backbone. <code>params.patch_size</code> (int) Patch size for MobileViT blocks. <code>params.num_attention_heads</code> (int) The number of heads in the multi-head attention. <code>params.attention_dropout_prob</code> (float) Dropout probability in the attention. <code>params.ffn_dropout_prob</code> (float) Dropout probability in the feed-forward network inside of the attention block. <code>params.output_expansion_ratio</code> (int) Expansion ratio for computing output dimension of the model. If expanded dimension is bigger than 960, it is set to 960. <code>params.use_fusion_layer</code> (bool) Whether to use fusion layer for MobileViT blocks. <code>stage_params[n].block_type</code> (str) Determines which block to use, \"mv2\" or \"mobilevit\". <code>stage_params[n].out_channels</code> (int) Output dimension of the block. <code>stage_params[n].num_blocks</code> (int) The number of blocks in the stage. Note that if <code>block_type</code> is <code>mobilevit</code>, an extra inverted residual block is added before MobileViT blocks. <code>stage_params[n].stride</code> (int) Stride value for the block. <code>stage_params[n].attention_channels</code> (int) Dimension for the attention block. If is used only <code>block_type</code> is \"mobilevit\". <code>stage_params[n].ffn_intermediate_channels</code> (int) Intermediate dimension for the feed forward network inside of the attention block. <code>stage_params[n].dilate</code> (bool) Whether to replace stride as dilated convolution. It is used only <code>block_type</code> is <code>mobilevit</code>. <code>stage_params[n].ir_expansion_ratio</code> (int) Dimension expansion ratio for inverted residual block."},{"location":"models/backbones/mobilevit/#model-configuration-examples","title":"Model configuration examples","text":"MobileViT-xxs <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilevit\n      params:\n        patch_size: 2\n        num_attention_heads: 4  # num_heads\n        attention_dropout_prob: 0.1\n        ffn_dropout_prob: 0.0\n        output_expansion_ratio: 4\n        use_fusion_layer: True\n      stage_params:\n        -\n          block_type: 'mv2'\n          out_channels: 16\n          num_blocks: 1\n          stride: 1\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mv2'\n          out_channels: 24\n          num_blocks: 3\n          stride: 2\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mobilevit'\n          out_channels: 48\n          num_blocks: 2\n          stride: 2\n          attention_channels: 64\n          ffn_intermediate_channels: 128\n          dilate: False\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mobilevit'\n          out_channels: 64\n          num_blocks: 4\n          stride: 2\n          attention_channels: 80\n          ffn_intermediate_channels: 160\n          dilate: False\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mobilevit'\n          out_channels: 80\n          num_blocks: 3\n          stride: 2\n          attention_channels: 96\n          ffn_intermediate_channels: 192\n          dilate: False\n          ir_expansion_ratio: 2\n</code></pre> MobileViT-xs <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilevit\n      params:\n        patch_size: 2\n        num_attention_heads: 4  # num_heads\n        attention_dropout_prob: 0.1\n        ffn_dropout_prob: 0.0\n        output_expansion_ratio: 4\n        use_fusion_layer: True\n      stage_params:\n        -\n          block_type: 'mv2'\n          out_channels: 32\n          num_blocks: 1\n          stride: 1\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mv2'\n          out_channels: 48\n          num_blocks: 3\n          stride: 2\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 64\n          num_blocks: 2\n          stride: 2\n          attention_channels: 96\n          ffn_intermediate_channels: 192\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 80\n          num_blocks: 4\n          stride: 2\n          attention_channels: 120\n          ffn_intermediate_channels: 240\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 96\n          num_blocks: 3\n          stride: 2\n          attention_channels: 144\n          ffn_intermediate_channels: 288\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n</code></pre> MobileViT-s <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilevit\n      params:\n        patch_size: 2\n        num_attention_heads: 4  # num_heads\n        attention_dropout_prob: 0.1\n        ffn_dropout_prob: 0.0\n        output_expansion_ratio: 4\n        use_fusion_layer: True\n      stage_params:\n        -\n          block_type: 'mv2'\n          out_channels: 32\n          num_blocks: 1\n          stride: 1\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mv2'\n          out_channels: 64\n          num_blocks: 3\n          stride: 2\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 96\n          num_blocks: 2\n          stride: 2\n          attention_channels: 144\n          ffn_intermediate_channels: 288\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 128\n          num_blocks: 4\n          stride: 2\n          attention_channels: 192\n          ffn_intermediate_channels: 384\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 160\n          num_blocks: 3\n          stride: 2\n          attention_channels: 240\n          ffn_intermediate_channels: 480\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n</code></pre>"},{"location":"models/backbones/mobilevit/#related-links","title":"Related links","text":"<ul> <li><code>apple/ml-cvnets</code></li> </ul>"},{"location":"models/backbones/resnet/","title":"ResNet","text":"<p>ResNet backbone based on Deep Residual Learning for Image Recognition.</p> <p>You can flexibly choose between basicblock and bottleneck as the building blocks for the ResNet architecture. And, you can also freely determine the number of stages and the repetition of blocks within the model. This flexibility supports the creation of various ResNet models, e.g. ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152. Also this supports adjusting the number of stages and blocks for your specific requirements.</p>"},{"location":"models/backbones/resnet/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FPN       YOLOPAFPN             FC       ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/backbones/resnet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"resnet\" to use <code>ResNet</code> backbone. <code>params.block_type</code> (str) Key value that determines which block to use, \"basicblock\" or \"bottleneck\". <code>params.norm_type</code> (str) Type of normalization layer. Supporting normalization layers are described in [here]. <code>stage_params[n].channels</code> (int) The dimension of the first convolution layer in each block. <code>stage_params[n].num_blocks</code> (int) The number of blocks in the stage. <code>stage_params[n].replace_stride_with_dilation</code> (bool) Flag that determines whether to replace stride step with dilated convolution."},{"location":"models/backbones/resnet/#model-configuration-examples","title":"Model configuration examples","text":"ResNet18 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: basicblock\n        norm_layer: batch_norm\n      stage_params:\n        - \n          channels: 64\n          layers: 2\n        - \n          channels: 128\n          layers: 2\n          replace_stride_with_dilation: False\n        - \n          channels: 256\n          layers: 2\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 2\n          replace_stride_with_dilation: False\n</code></pre> ResNet34 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: basicblock\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 4\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 6\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre> ResNet50 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: bottleneck\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 4\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 6\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre> ResNet101 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: bottleneck\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 4\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 23\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre> ResNet152 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: bottleneck\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 8\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 36\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre>"},{"location":"models/backbones/resnet/#related-links","title":"Related links","text":"<ul> <li><code>pytorch/vision</code></li> </ul>"},{"location":"models/backbones/vit/","title":"ViT","text":"<p>ViT backbone based on An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</p> <p>ViT (Vision Transformer) does not have a stage configuration and therefore does not support compatibility with neck modules. Currently, it only supports the FC head. When using the ViT model for classification tasks, users can decide whether to use a classification token. Additionally, users can flexibly configure the settings of the transformer encoder.</p>"},{"location":"models/backbones/vit/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting necks Supporting heads torch.fx NetsPresso        FC Supported Supported"},{"location":"models/backbones/vit/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"vit\" to use <code>ViT</code> backbone. <code>params.patch_size</code> (int) Size of the image patch to be treated as a single embedding. <code>params.attention_channels</code> (int) Dimension for the encoder. <code>params.num_blocks</code> (int) The number of self-attention blocks in the encoder. <code>params.num_attention_heads</code> (int) The number of heads in the multi-head attention. <code>params.attention_dropout_prob</code> (float) Dropout probability in the attention block. <code>params.ffn_intermediate_channels</code> (int) Intermediate dimension of the feed-forward network inside the attention block. <code>params.ffn_dropout_prob</code> (float) Dropout probability in the feed-forward network inside the attention block. <code>params.use_cls_token</code> (bool) Whether to use the classification token. <code>params.vocab_size</code> (int) Maximum token length for positional encoding."},{"location":"models/backbones/vit/#model-configuration-examples","title":"Model configuration examples","text":"ViT-tiny <pre><code>model:\n  architecture:\n    backbone:\n      name: vit\n      params:\n        patch_size: 16\n        attention_channels: 192\n        num_blocks: 12\n        num_attention_heads: 3\n        attention_dropout_prob: 0.0\n        ffn_intermediate_channels: 768  # hidden_size * 4\n        ffn_dropout_prob: 0.1\n        use_cls_token: True\n        vocab_size: 1000\n      stage_params: ~\n</code></pre> ViT-small <pre><code>model:\n  architecture:\n    backbone:\n      name: vit\n      params:\n        patch_size: 16\n        attention_channels: 384\n        num_blocks: 12\n        num_attention_heads: 6\n        attention_dropout_prob: 0.0\n        ffn_intermediate_channels: 1536  # hidden_size * 4\n        ffn_dropout_prob: 0.0\n        use_cls_token: True\n        vocab_size: 1000\n      stage_params: ~\n</code></pre>"},{"location":"models/backbones/vit/#related-links","title":"Related links","text":"<ul> <li><code>apple/ml-cvnets</code></li> </ul>"},{"location":"models/fullmodels/pidnet/","title":"PIDNet","text":"<p>PIDNet model based on PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers.</p>"},{"location":"models/fullmodels/pidnet/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting Task(s) torch.fx NetsPresso Segmentation Supported Supported"},{"location":"models/fullmodels/pidnet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"pidnet\" to use PIDNet model. <code>m</code> (int) Residual block repetetion count for 1, 2 stage of I branch, and 3, 4 stage of P branch. <code>n</code> (int) Residual block repetition count for 3, 4 stage of I branch <code>channels</code> (int) Base dimension of the overall model except ppm and head module. <code>ppm_channels</code> (int) Dimension of the ppm module. <code>head_channels</code> (int) Dimension of the head module. PIDNet-s <pre><code>model:\n  architecture:\n    full:\n      name: pidnet\n      m: 2\n      n: 3\n      channels: 32\n      ppm_channels: 96\n      head_channels: 128\n</code></pre> PIDNet-m <pre><code>model:\n  architecture:\n    full:\n      name: pidnet\n      m: 2\n      n: 3\n      channels: 64\n      ppm_channels: 96\n      head_channels: 128\n</code></pre> PIDNet-l <pre><code>model:\n  architecture:\n    full:\n      name: pidnet\n      m: 3\n      n: 4\n      channels: 64\n      ppm_channels: 112\n      head_channels: 256\n</code></pre>"},{"location":"models/fullmodels/pidnet/#related-links","title":"Related links","text":"<ul> <li><code>XuJiacong/PIDNet</code></li> </ul>"},{"location":"models/heads/allmlpdecoder/","title":"AllMLPDecoder","text":"<p>All-MLP decoder based on SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.</p> <p>We provide the AllMLP Decoder, the head of SegFormer, as a freely usable head module. AllMLP Decoder takes intermediate features from previous backbone or neck module and outputs a segmentation map of the target size.</p>"},{"location":"models/heads/allmlpdecoder/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting necks torch.fx NetsPresso        ResNet       MobileNetV3       MixNet       CSPDarkNet       MobileViT       MixTransformer       EfficientFormer           FPN     YOLOPAFPN Supported Supported"},{"location":"models/heads/allmlpdecoder/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"all_mlp_decoder\" to use <code>AllMLPDecoder</code> head. <code>params.intermediate_channels</code> (int) Intermediate feature dimension of the decoder. <code>params.classifier_dropout_prob</code> (float) Dropout probability of classifier."},{"location":"models/heads/allmlpdecoder/#model-configuration-example","title":"Model configuration example","text":"AllMLP decoder <pre><code>model:\n  architecture:\n    head:\n      name: all_mlp_decoder\n      params:\n        intermediate_channels: 256\n        classifier_dropout_prob: 0.\n</code></pre>"},{"location":"models/heads/allmlpdecoder/#related-links","title":"Related links","text":""},{"location":"models/heads/anchordecoupledhead/","title":"AnchorDecoupledHead","text":"<p>Decoupled detection head with anchors based on Focal Loss for Dense Object Detection</p> <p>We have named the detection head of RetinaNet as AnchorDecoupledHead to represent it in a more general term. AnchorDecoupledHead consists of a box regression head and a classification head for the given intermediate features, predicting detection boxes for the anchors at each feature's pixel location. Additionally, we provide the option to adjust the number of convolution layers used in the heads through the <code>num_layers</code> value, it is equivalent with RetinaNet when set to 4.</p>"},{"location":"models/heads/anchordecoupledhead/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting necks torch.fx NetsPresso        ResNet       MobileNetV3       MixNet       CSPDarkNet       MobileViT       MixTransformer       EfficientFormer           FPN     YOLOPAFPN Supported Supported"},{"location":"models/heads/anchordecoupledhead/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"retinanet_head\" to use <code>RetinaNetHead</code> head. <code>params.anchor_sizes</code> (list[list[int]]) Default anchor sizes for each intermediate feature. <code>params.aspect_ratios</code> (list[float]) List of aspect ratio for each anchor. <code>params.num_layers</code> (int) The number of convolution layers of regression and classification head. <code>params.norm_layer</code> (str) Normalization type for the head. <code>params.topk_candidates</code> (int) The number of boxes to retain based on score during the decoding step. <code>params.score_thresh</code> (float) Score thresholding value applied during the decoding step. <code>params.nms_thresh</code> (float) IoU threshold for non-maximum suppression. <code>params.class_agnostic</code> (bool) Whether to process class-agnostic non-maximum suppression."},{"location":"models/heads/anchordecoupledhead/#model-configuration-example","title":"Model configuration example","text":"Anchor-based decoupled detection head <pre><code>model:\n  architecture:\n    head:\n      name: anchor_decoupled_head\n      params:\n        anchor_sizes: [[32,], [64,], [128,], [256,]]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        num_layers: 1\n        norm_type: batch_norm\n        # postprocessor - decode\n        topk_candidates: 1000\n        score_thresh: 0.05\n        # postprocessor - nms\n        nms_thresh: 0.45\n        class_agnostic: False\n</code></pre>"},{"location":"models/heads/anchordecoupledhead/#related-links","title":"Related links","text":"<ul> <li><code>pytorch/vision</code></li> </ul>"},{"location":"models/heads/anchorfreedecoupledhead/","title":"AnchorFreeDecoupledHead","text":"<p>Anchor-free decoupled detection head based on YOLOX: Exceeding YOLO Series in 2021.</p> <p>We provide the head of YOLOX as AnchorFreeDecoupledHead. There are no differnece with the original model, and currently, it is set to pass non-maximum suppression function.</p>"},{"location":"models/heads/anchorfreedecoupledhead/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting necks torch.fx NetsPresso        ResNet       MobileNetV3       MixNet       CSPDarkNet       MobileViT       MixTransformer       EfficientFormer           FPN     YOLOPAFPN Supported Supported"},{"location":"models/heads/anchorfreedecoupledhead/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"yolox_head\" to use <code>YOLOX</code> head. <code>params.act_type</code> (float) Activation function for the head. <code>params.score_thresh</code> (float) Score thresholding value applied during the decoding step. <code>params.class_agnostic</code> (bool) Whether to process class-agnostic non-maximum suppression."},{"location":"models/heads/anchorfreedecoupledhead/#model-configuration-example","title":"Model configuration example","text":"Anchor-free decoupled detection head <pre><code>model:\n  architecture:\n    head:\n      name: anchor_free_decoupled_head\n      params:\n        act_type: \"silu\"\n        # postprocessor - decode\n        score_thresh: 0.7\n        # postprocessor - nms\n        nms_thresh: 0.45\n        class_agnostic: False\n</code></pre>"},{"location":"models/heads/anchorfreedecoupledhead/#related-links","title":"Related links","text":"<ul> <li><code>Megvii-BaseDetection/YOLOX</code></li> </ul>"},{"location":"models/heads/fc/","title":"FC","text":"<p>Fully connected layer head for classification. You can adjust the number of layers and channel sizes. Channel of last layer is always same with the number of classes.</p>"},{"location":"models/heads/fc/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting necks torch.fx NetsPresso        ResNet       MobileNetV3       MixNet       CSPDarkNet       ViT       MobileViT       MixTransformer       EfficientFormer      Supported Supported"},{"location":"models/heads/fc/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"fc\" to use <code>FC</code> head. <code>params.num_layers</code> (int) The number of fully connected layers. Channel of last layer is same with the number of classes. <code>params.intermediate_channels</code> (int) Dimension of intermediate fully connected layers. This can be ignored if <code>num_layer</code> is 1. <code>params.act_type</code> (str) Activation function for intermediate fully connected layers. This can be ignored if <code>num_layer</code> is 1. <code>params.dropout_prob</code> (float) Dropout probability before the last classifier layer."},{"location":"models/heads/fc/#model-configuration-example","title":"Model configuration example","text":"2-layer fully connected layer classifier <pre><code>model:\n  architecture:\n    head:\n      name: fc\n      params:\n        num_layers: 2\n        intermediate_channels: 1024\n        act_type: hard_swish\n        dropout_prob: 0.2\n</code></pre>"},{"location":"models/heads/fc/#related-links","title":"Related links","text":""},{"location":"models/heads/rtmcc/","title":"RTMCC","text":"<p>RTMCC head based on RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose.</p>"},{"location":"models/heads/rtmcc/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting necks torch.fx NetsPresso To be updated ... To be updated ... To be updated ... To be updated ..."},{"location":"models/heads/rtmcc/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"rtmcc\" to use <code>RTMCC</code> head. <code>params.conv_kernel</code> (int) Kernel size of convolution layer. <code>params.attention_channels</code> (int) Dimension of gated attention unit. <code>params.attention_act_type</code> (str) Activation type of gated attention unit. <code>params.attention_pos_enc</code> (bool) Whether to use rotary position embedding for gated attention unit. <code>params.s</code> (int) Self attention feature dimension of gated attention unit. <code>params.expansion_factor</code> (int) Expansion factor of gated attention unit. <code>params.dropout_rate</code> (float) Dropout rate of gated attention unit. <code>params.drop_path</code> (float) Drop path rate of gated attention unit. <code>params.use_rel_bias</code> (bool) Whether to use relative bias for gated attention unit. <code>params.simcc_split_ratio</code> (float) Split ratio of pixels. <code>params.target_size</code> (list) Original input image size. <code>params.backbone_stride</code> (int) Stride of input feature from original image produced by backbone."},{"location":"models/heads/rtmcc/#model-configuration-example","title":"Model configuration example","text":"RTMCC head <pre><code>model:\n  architecture:\n    head:\n    name: rtmcc\n    params:\n      conv_kernel: 7\n      attention_channels: 256\n      attention_act_type: 'silu'\n      attention_pos_enc: False\n      s: 128\n      expansion_factor: 2\n      dropout_rate: 0.\n      drop_path: 0.\n      use_rel_bias: False\n      simcc_split_ratio: 2.\n      target_size: [256, 256]\n      backbone_stride: 32\n</code></pre>"},{"location":"models/heads/rtmcc/#related-links","title":"Related links","text":"<ul> <li><code>RTMPose</code></li> </ul>"},{"location":"models/layers/activations/","title":"Activations","text":"<p>Choosing Activation functions is a key part of neural network design, since they determines the output of nodes in the network. They introduce non-linear properties to the network, enabling it to learn complex data patterns and make more sophisticated predictions.</p> <p>To enable diverse model designs, NetsPresso Trainer supports various activation modules based on the pytorch library.</p>"},{"location":"models/layers/activations/#supporting-activation-functions","title":"Supporting activation functions","text":"<p>The currently supported activation functions in NetsPresso Trainer are as follows.</p>"},{"location":"models/layers/activations/#relu","title":"ReLU","text":"<ul> <li>This can be applied by giving <code>'relu'</code> keyword</li> <li>ReLU activation follows the torch.nn.ReLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#prelu","title":"PReLU","text":"<ul> <li>This can be applied by giving <code>'prelu'</code> keyword</li> <li>PReLU activation follows the torch.nn.PReLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li>This can be applied by giving <code>'leaky_relu'</code> keyword</li> <li>Leaky ReLU activation follows the torch.nn.LeakyReLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#gelu","title":"GELU","text":"<ul> <li>This can be applied by giving <code>'gelu'</code> keyword</li> <li>GELU activation follows the torch.nn.GELU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#silu","title":"SiLU","text":"<ul> <li>This can be applied by giving <code>'silu'</code> or <code>'swish'</code>keyword</li> <li>SiLU activation follows the torch.nn.SiLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#hardswish","title":"Hardswish","text":"<ul> <li>This can be applied by giving <code>'hard_swish'</code> keyword</li> <li>Hardswish activation follows the torch.nn.Hardswish in the PyTorch library.</li> </ul>"},{"location":"models/layers/normalizations/","title":"Normalizations","text":"<p>Normalization layers significantly affect model performance by transforming features to similar scales. To enable diverse model designs, NetsPresso Trainer supports various activation modules based on the pytorch library.</p>"},{"location":"models/layers/normalizations/#supporting-normalization-layers","title":"Supporting normalization layers","text":"<p>The currently supported normalization layers in NetsPresso Trainer are as follows.</p>"},{"location":"models/layers/normalizations/#batchnorm","title":"BatchNorm","text":"<ul> <li>This can be applied by giving <code>'batch_norm'</code> keyword</li> <li>Batch normalization follows torch.nn.BatchNorm2d in the PyTorch library.</li> </ul>"},{"location":"models/layers/normalizations/#instancenorm","title":"InstanceNorm","text":"<ul> <li>This can be applied by giving <code>'instance_norm'</code> keyword</li> <li>Instance normalization follows the torch.nn.InstanceNorm2d in the PyTorch library</li> </ul>"},{"location":"models/necks/fpn/","title":"FPN","text":"<p>FPN based on Feature Pyramid Networks for Object Detection</p> <p>The Feature Pyramid Network (FPN) is designed to enhance feature maps given from the backbone, typically used for detection models. Therefore, we also recommend to use it in detection task as well. FPN can create more pyramid deeply than the input feature pyramid from backbone, and in such cases, additional convolution or pooling layers are added.</p>"},{"location":"models/necks/fpn/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting heads torch.fx NetsPresso        ResNet       MobileNetV3       MixNet       CSPDarkNet       MobileViT       MixTransformer       EfficientFormer             ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/necks/fpn/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"fpn\" to use <code>FPN</code> neck. <code>params.num_outs</code> (int) The number of output feature maps. This must greater than or equal to the number of input feature maps. If <code>end_level</code> is not the last feature map produced by the backbone, extra levels that generated by setting <code>num_outs</code> beyond the number of input feature maps are not allowed. <code>params.start_level</code> (int) Determines the starting index from the list of feature maps produced by the backbone. It defines the number of input feature maps with <code>end_level</code>. <code>params.end_level</code> (int) Determines the end index from the list of feature maps produced by the backbone.  If -1, it equals to using up to the last feature map. it defines the number of input feature maps with <code>start_level</code>. <code>params.add_extra_convs</code> (str) Defines additional convolution layers for pyramid construction when <code>num_outs</code> is greater than the number of input feature maps. Options are <code>on_input</code>, <code>on_lateral</code>, <code>on_output</code>. If <code>None</code>, max pooling is applied. <code>params.relu_before_extra_convs</code> (bool) Determines whether to apply the <code>relu</code> activation function to the extra convolutions that are generated."},{"location":"models/necks/fpn/#model-configuration-examples","title":"Model configuration examples","text":"FPN (4-stage -&gt; 4-stage) <pre><code>model:\n  architecture:\n    neck:\n      name: fpn\n      params:\n        num_outs: 4\n        start_level: 0\n        end_level: -1\n        add_extra_convs: False\n        relu_before_extra_convs: False\n</code></pre>"},{"location":"models/necks/fpn/#related-links","title":"Related links","text":""},{"location":"models/necks/yolopafpn/","title":"YOLOPAFPN","text":"<p>YOLOPAFPN based on YOLOX: Exceeding YOLO Series in 2021.</p> <p>YOLOPAFPN is a modified PAFPN for YOLOX model. Therefore, although YOLOPAFP is compatible with various backbones, we recommend to use it when constructing YOLOX models. The size is determined by <code>dep_mul</code> value, which defines the repetition of CSPLayers.</p>"},{"location":"models/necks/yolopafpn/#compatibility-matrix","title":"Compatibility matrix","text":"Supporting backbones Supporting heads torch.fx NetsPresso        ResNet       MobileNetV3       MixNet       CSPDarkNet       MobileViT       MixTransformer       EfficientFormer             ALLMLPDecoder       AnchorDecoupledHead       AnchorFreeDecoupledHead      Supported Supported"},{"location":"models/necks/yolopafpn/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"yolopafpn\" to use <code>YOLOPAFPN</code> neck. <code>params.dep_mul</code> (int) Multiplying factor determining the repetition count of <code>CSPLayer</code> in the backbone. <code>params.act_type</code> (int) Type of activation function for the model. Supporting activation functions are described in [here]."},{"location":"models/necks/yolopafpn/#model-configuration-examples","title":"Model configuration examples","text":"PAFPN for YOLOX-s <pre><code>model:\n  architecture:\n    neck:\n      name: yolopafpn\n      params:\n        dep_mul: 0.33\n        act_type: \"silu\"\n</code></pre>"},{"location":"models/necks/yolopafpn/#related-links","title":"Related links","text":"<ul> <li><code>Megvii-BaseDetection/YOLOX</code></li> </ul>"}]}